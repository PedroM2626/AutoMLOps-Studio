import os
import logging
import importlib.util

# Reduce TensorFlow noise
os.environ["TF_ENABLE_ONEDNN_OPTS"] = "0"

# Check if transformers and torch are installed without importing them yet
# This prevents potential DLL conflicts/crashes during initial module load
try:
    _transformers_spec = importlib.util.find_spec("transformers")
    _torch_spec = importlib.util.find_spec("torch")
    TRANSFORMERS_AVAILABLE = (_transformers_spec is not None) and (_torch_spec is not None)
    if TRANSFORMERS_AVAILABLE:
        print("DEBUG: Transformers/Torch detected (lazy loading enabled).")
    else:
        print("DEBUG: Transformers/Torch not found.")
except Exception as e:
    TRANSFORMERS_AVAILABLE = False
    print(f"DEBUG: Error checking transformers availability: {e}")

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score, TimeSeriesSplit, KFold, StratifiedKFold
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, OneHotEncoder, LabelEncoder, OrdinalEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import (
    RandomForestClassifier, RandomForestRegressor, 
    GradientBoostingClassifier, GradientBoostingRegressor, 
    VotingClassifier, VotingRegressor, IsolationForest,
    ExtraTreesClassifier, ExtraTreesRegressor,
    AdaBoostClassifier, AdaBoostRegressor,
    StackingClassifier, StackingRegressor
)
from sklearn.neighbors import LocalOutlierFactor, KNeighborsClassifier, KNeighborsRegressor
from sklearn.linear_model import (
    LogisticRegression, LinearRegression, Ridge, Lasso, ElasticNet, 
    SGDClassifier, SGDRegressor, RidgeClassifier, PassiveAggressiveClassifier
)
from sklearn.svm import SVC, SVR, OneClassSVM, LinearSVC
from sklearn.naive_bayes import GaussianNB, BernoulliNB
from sklearn.covariance import EllipticEnvelope
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor
from sklearn.neural_network import MLPClassifier, MLPRegressor
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, 
    mean_squared_error, mean_absolute_error, r2_score, confusion_matrix,
    silhouette_score, mean_absolute_percentage_error, balanced_accuracy_score,
    cohen_kappa_score, log_loss, matthews_corrcoef, explained_variance_score,
    median_absolute_error, mean_squared_log_error, calinski_harabasz_score,
    davies_bouldin_score
)
from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN, MeanShift, Birch, SpectralClustering
from sklearn.mixture import GaussianMixture
from sklearn.base import BaseEstimator, ClassifierMixin, RegressorMixin
import xgboost as xgb
import lightgbm as lgb
import re
try:
    import catboost as cb
    CATBOOST_AVAILABLE = True
except ImportError:
    CATBOOST_AVAILABLE = False
import optuna
import joblib
import os
import logging
import time
import warnings
# import matplotlib.pyplot as plt # Moved to local scope
# import seaborn as sns # Moved to local scope
import mlflow
from sklearn.exceptions import ConvergenceWarning
from sklearn.model_selection import cross_val_predict

# Transformers import moved to top of file

# Silenciar avisos de converg√™ncia e outros avisos repetitivos
warnings.filterwarnings("ignore", category=ConvergenceWarning)
warnings.filterwarnings("ignore", category=UserWarning)

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class TransformersWrapper(BaseEstimator, ClassifierMixin, RegressorMixin):
    def __init__(self, model_name='bert-base-uncased', task='classification', epochs=3, learning_rate=2e-5):
        self.model_name = model_name
        self.task = task
        self.epochs = epochs
        self.learning_rate = learning_rate
        self.model = None
        self.tokenizer = None
        self.device = None # Lazy initialization to avoid early torch import

    def fit(self, X, y=None):
        logger.info(f"TransformersWrapper.fit called for {self.model_name}")
        if not TRANSFORMERS_AVAILABLE:
            raise ImportError("Transformers library not found.")
            
        try:
            logger.info("Importing torch/transformers...")
            import torch
            from transformers import AutoModelForSequenceClassification, AutoTokenizer
            logger.info("Imports successful.")
        except ImportError as e:
            logger.error(f"Failed to import transformers/torch at runtime: {e}")
            raise e
            
        if self.device is None:
            logger.info("Checking CUDA availability...")
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
            logger.info(f"Device selected: {self.device}")
        
        # Check if input is likely vectorized (sparse or dense matrix)
        # Handle 'passthrough' mode where X might be a numpy array of strings
        is_vectorized = False
        is_raw_text = False
        
        logger.info(f"Checking input type: {type(X)}")
        if hasattr(X, "shape"):
             logger.info(f"Checking input shape: {X.shape}")
             
             # If it's an array of strings/objects, it's raw text
             if X.dtype == 'object' or X.dtype.type is np.str_ or X.dtype.type is np.bytes_:
                 is_raw_text = True
                 logger.info("Input detected as Raw Text (dtype object/string).")
             elif len(X.shape) > 1 and X.shape[1] > 1:
                # Simple heuristic: if it has many columns, it's likely a feature matrix
                is_vectorized = True
                logger.info("Input detected as Vectorized Data.")
            
        if is_vectorized and not is_raw_text:
            logger.warning(f"TransformersWrapper ({self.model_name}) received vectorized data (shape {X.shape}). Skipping fine-tuning as it requires raw text.")
            
            # Use a lightweight dummy model or just leave as None and handle in predict
            self.model = None 
            logger.info("Skipping training due to vectorized input.")
            return self

        logger.info("Proceeding with training (raw text assumption)...")
        # If we somehow got text (e.g. custom pipeline), we would proceed here
        
        # Ensure X is a list of strings for tokenizer
        if hasattr(X, "tolist"):
             texts = X.tolist()
        else:
             texts = list(X)
             
        # Flatten if list of lists (common with reshape(-1, 1))
        if len(texts) > 0 and isinstance(texts[0], (list, np.ndarray)):
            texts = [t[0] for t in texts]
            
        # Convert to string just in case
        texts = [str(t) for t in texts]
        
        # For now, just load model to ensure connectivity
        num_labels = len(np.unique(y)) if y is not None and self.task == 'classification' else 1
        logger.info(f"Loading tokenizer and model {self.model_name}...")
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        self.model = AutoModelForSequenceClassification.from_pretrained(self.model_name, num_labels=num_labels).to(self.device)
        
        # Mock Training Loop (Short)
        logger.info("Simulating training loop (1 batch)...")
        # In a real implementation, we would tokenize and train here.
        # inputs = self.tokenizer(texts, padding=True, truncation=True, return_tensors="pt").to(self.device)
        # ... training code ...
        
        logger.info(f"TransformersWrapper: Loaded {self.model_name} on {self.device}")
        return self

    def predict(self, X):
        if self.model is None:
            # If model wasn't trained (e.g. due to vectorized input), raise error 
            # so AutoMLTrainer catches it and assigns bad score, rather than random.
            raise RuntimeError("TransformersWrapper model is not trained (likely due to vectorized input).")
            
        # Mock prediction for interface testing
        return np.zeros(len(X)) if self.task == 'regression' else np.random.randint(0, 2, size=len(X))

class AutoMLDataProcessor:
    def __init__(self, target_column=None, task_type=None, date_col=None, forecast_horizon=1, nlp_config=None, scaler_type='standard'):
        self.target_column = target_column
        self.task_type = task_type
        self.date_col = date_col
        self.forecast_horizon = forecast_horizon
        self.nlp_config = nlp_config if nlp_config else {}
        self.scaler_type = scaler_type
        self.preprocessor = None
        self.nlp_cols = []

    def _clean_text_feature(self, df, col):
        """Applies text cleaning to a specific column in DataFrame."""
        if col in df.columns:
            cleaning_mode = self.nlp_config.get('cleaning_mode', 'standard')
            logger.info(f"üßπ Limpando texto da coluna: {col} (Mode: {cleaning_mode})")
            
            def clean_text_optimized(text):
                text = str(text).lower()
                # Remover URLs
                text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)
                # Remover men√ß√µes e hashtags
                text = re.sub(r'\@\w+|\#','', text)
                
                if cleaning_mode == 'god_mode':
                    # Remove repetitive characters (e.g., goooood -> good)
                    text = re.sub(r'(.)\1+', r'\1\1', text)
                    # Keep punctuation that matters for sentiment (! and ?)
                    text = re.sub(r'[^a-z\s\!\?]', '', text)
                else:
                    # Standard cleaning: only letters and spaces
                    text = re.sub(r'[^a-z\s]', '', text)
                
                # Remover espa√ßos extras
                text = " ".join(text.split())
                return text
            
            # Apply cleaning in-place (efficiently)
            df[col] = df[col].apply(clean_text_optimized)
        return df

    def _apply_ts_features(self, df, y=None):
        """Applies time-series specific feature engineering to a DataFrame."""
        df = df.copy()
        
        # 1. Temporal Features
        if self.date_col and self.date_col in df.columns:
            try:
                df[self.date_col] = pd.to_datetime(df[self.date_col])
                df['hour'] = df[self.date_col].dt.hour
                df['dayofweek'] = df[self.date_col].dt.dayofweek
                df['quarter'] = df[self.date_col].dt.quarter
                df['month'] = df[self.date_col].dt.month
                df['year'] = df[self.date_col].dt.year
                df['dayofyear'] = df[self.date_col].dt.dayofyear
                df['dayofmonth'] = df[self.date_col].dt.day
                df['weekofyear'] = df[self.date_col].dt.isocalendar().week.astype(int)
            except Exception as e:
                logger.warning(f"Could not extract temporal features: {e}")

        # 2. Lag Features & Rolling Stats
        target_vals = None
        if y is not None:
            target_vals = y
        elif self.target_column and self.target_column in df.columns:
            target_vals = df[self.target_column]
            
        if target_vals is not None:
            target_vals_numeric = pd.to_numeric(target_vals, errors='coerce')
            if not target_vals_numeric.isna().all():
                target_vals = target_vals_numeric
                if self.target_column and self.target_column in df.columns:
                    df[self.target_column] = target_vals
                
                for i in range(self.forecast_horizon, self.forecast_horizon + 5):
                    df[f'lag_{i}'] = target_vals.shift(i)
                
                df[f'rolling_mean_{self.forecast_horizon}'] = target_vals.shift(self.forecast_horizon).rolling(window=3).mean()
                df[f'rolling_std_{self.forecast_horizon}'] = target_vals.shift(self.forecast_horizon).rolling(window=3).std()
                df = df.dropna()
            
        return df

    def _apply_nlp_features(self, df, nlp_cols):
        """Deprecated: NLP features are now handled in the main pipeline."""
        return df

    def fit_transform(self, df, nlp_cols=None):
        self.nlp_cols = nlp_cols if nlp_cols else []
        
        # NLP Cleaning first
        if self.nlp_cols:
            for col in self.nlp_cols:
                df = self._clean_text_feature(df, col)
                # Fill NaNs for vectorizer
                if col in df.columns:
                     df[col] = df[col].fillna("")

        # Time Series Feature Engineering
        if self.task_type == 'time_series':
            df = self._apply_ts_features(df)

        if self.target_column and self.target_column in df.columns:
            X = df.drop(columns=[self.target_column])
            y = df[self.target_column]
        else:
            X = df
            y = None
        
        # Exclude date_col from processing
        process_cols = [c for c in X.columns if c != self.date_col]
        
        nlp_features = [c for c in self.nlp_cols if c in process_cols]
        non_nlp_cols = [c for c in process_cols if c not in nlp_features]
        
        X_to_process = X[non_nlp_cols]
        
        # Identificar tipos de colunas
        numeric_features = X_to_process.select_dtypes(include=['int64', 'float64', 'int32', 'float32']).columns.tolist()
        all_categorical = X_to_process.select_dtypes(include=['object', 'category']).columns.tolist()

        # Drop constant columns
        constant_cols = [col for col in X_to_process.columns if X_to_process[col].nunique() <= 1]
        if constant_cols:
            numeric_features = [c for c in numeric_features if c not in constant_cols]
            all_categorical = [c for c in all_categorical if c not in constant_cols]
        
        # Split categorical
        low_card_features = []
        high_card_features = []
        
        for col in all_categorical:
            if X_to_process[col].nunique() <= 15:
                low_card_features.append(col)
            else:
                high_card_features.append(col)

        # Preprocessing Pipelines
        if self.scaler_type == 'minmax':
            scaler = MinMaxScaler()
        elif self.scaler_type == 'robust':
            scaler = RobustScaler()
        else:
            scaler = StandardScaler()

        numeric_transformer = Pipeline(steps=[
            ('imputer', SimpleImputer(strategy='median')),
            ('scaler', scaler)
        ])

        low_card_transformer = Pipeline(steps=[
            ('imputer', SimpleImputer(strategy='most_frequent')),
            ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))
        ])
        
        high_card_transformer = Pipeline(steps=[
            ('imputer', SimpleImputer(strategy='most_frequent')),
            ('ordinal', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))
        ])

        # Bundle preprocessing
        transformers = []
        if numeric_features:
            transformers.append(('num', numeric_transformer, numeric_features))
        if low_card_features:
            transformers.append(('cat_low', low_card_transformer, low_card_features))
        if high_card_features:
            transformers.append(('cat_high', high_card_transformer, high_card_features))

        # NLP Transformers
        if nlp_features:
            from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
            
            vectorizer_type = self.nlp_config.get('vectorizer', 'tfidf')
            ngram_range = self.nlp_config.get('ngram_range', (1, 3))
            max_features = self.nlp_config.get('max_features', 5000) # Reduced from 20000 to 5000 for speed
            stop_words = 'english' if self.nlp_config.get('stop_words', True) else None
            
            for col in nlp_features:
                if vectorizer_type == 'count':
                    vectorizer = CountVectorizer(
                        max_features=max_features,
                        ngram_range=ngram_range,
                        stop_words=stop_words
                    )
                elif vectorizer_type == 'passthrough':
                     # Custom transformer to pass text as-is (for BERT)
                     from sklearn.preprocessing import FunctionTransformer
                     # Ensure we handle Series/DataFrame input correctly for FunctionTransformer
                     # We need to return a 2D array (n_samples, 1) or list of strings
                     def pass_text(x):
                         if hasattr(x, 'values'):
                             x = x.values
                         if hasattr(x, 'to_numpy'):
                             x = x.to_numpy()
                         return x.reshape(-1, 1)
                         
                     vectorizer = FunctionTransformer(pass_text, validate=False)
                else:
                    # Apply specific settings for god_mode if requested, or user config
                    is_god_mode = self.nlp_config.get('cleaning_mode') == 'god_mode'
                    
                    vectorizer = TfidfVectorizer(
                        max_features=max_features,
                        ngram_range=ngram_range,
                        stop_words=stop_words,
                        sublinear_tf=self.nlp_config.get('sublinear_tf', True), # Default True for TF-IDF usually good
                        strip_accents='unicode' if is_god_mode else None
                    )
                transformers.append((f'nlp_{col}', vectorizer, col))

        # Sparse Threshold - IMPORTANT: Keep sparse for NLP
        sparse_thresh = 1.0 if nlp_features else 0
        self.preprocessor = ColumnTransformer(transformers=transformers, sparse_threshold=sparse_thresh)

        try:
            X_processed = self.preprocessor.fit_transform(X)
        except Exception as e:
            logger.error(f"Erro no fit_transform: {e}")
            raise e
        
        if X_processed.shape[0] == 0:
            raise ValueError("O processamento resultou em 0 linhas.")
            
        # Ensure output is dense only if NO NLP features (to save memory)
        if not nlp_features and hasattr(X_processed, "toarray"):
            X_processed = X_processed.toarray()
            
        # Handle target encoding
        y_processed = None
        if y is not None:
            if y.dtype == 'object' or y.dtype.name == 'category':
                self.label_encoder = LabelEncoder()
                y_processed = self.label_encoder.fit_transform(y)
            else:
                y_processed = y

        return X_processed, y_processed

    def transform(self, df):
        # NLP Cleaning
        if self.nlp_cols:
            for col in self.nlp_cols:
                df = self._clean_text_feature(df, col)
                if col in df.columns:
                     df[col] = df[col].fillna("")

        # Time Series Feature Engineering
        if self.task_type == 'time_series':
            df = self._apply_ts_features(df)

        if df is None or (isinstance(df, pd.DataFrame) and df.empty):
            return None, None

        if self.target_column and self.target_column in df.columns:
            X = df.drop(columns=[self.target_column])
            y = df[self.target_column]
            # Handle categorical target
            if hasattr(self, 'label_encoder') and self.label_encoder:
                try:
                    y = self.label_encoder.transform(y)
                except:
                    pass 
        else:
            X = df
            y = None
        
        if not isinstance(X, pd.DataFrame):
            X = pd.DataFrame(X)

        try:
            X_processed = self.preprocessor.transform(X)
        except Exception as e:
            logger.error(f"Erro no ColumnTransformer.transform: {e}")
            raise e

        # Ensure dense only if NO NLP features
        if not self.nlp_cols and hasattr(X_processed, "toarray"):
            X_processed = X_processed.toarray()
            
        return X_processed, y

    def get_feature_names(self):
        """Returns the names of the features after preprocessing."""
        if self.preprocessor is None:
            return []
        
        feature_names = []
        for name, transformer, columns in self.preprocessor.transformers_:
            if name == 'remainder' and transformer == 'drop':
                continue
            
            if hasattr(transformer, 'get_feature_names_out'):
                # Para OneHotEncoder e outros que mudam o n√∫mero de colunas
                names = transformer.get_feature_names_out(columns)
                feature_names.extend(names)
            else:
                # Para StandardScaler, SimpleImputer, etc. que mant√™m o n√∫mero de colunas
                feature_names.extend(columns)
        
        return feature_names

class AutoMLTrainer:
    def __init__(self, task_type='classification', preset='medium', ensemble_config=None):
        self.task_type = task_type
        self.preset = preset
        self.ensemble_config = ensemble_config or {}
        self.best_model = None
        self.best_params = None
        self.results = []
        
        # Configura√ß√µes baseadas no preset
        self.preset_configs = {
            'fast': {
                'n_trials': 15,
                'timeout': 600,  # 10 min
                'cv': 3,
                'models': ['logistic_regression', 'random_forest', 'xgboost', 'decision_tree', 'ridge_classifier']
            },
            'medium': {
                'n_trials': 40,
                'timeout': 1800, # 30 min
                'cv': 5,
                'models': ['logistic_regression', 'random_forest', 'xgboost', 'lightgbm', 'extra_trees', 'svm', 'knn', 'mlp']
            },
            'best_quality': {
                'n_trials': 3, # Reduced to avoid long wait times
                'timeout': 3600, # 1 hour
                'cv': 5, # Reduced from 10 to 5 for speed
                # Use a representative subset for interface simulation speed
                'models': ['voting_ensemble', 'sgd_classifier', 'bert-base-uncased', 'xgboost']
                # Full list: ['logistic_regression', 'random_forest', 'xgboost', 'lightgbm', 'catboost', 'svm', 'mlp', 'extra_trees', 'adaboost', 'sgd_classifier', 'passive_aggressive', 'bert-base-uncased', 'distilbert-base-uncased', 'roberta-base']
            },
            'custom': {
                'n_trials': 20, # Default if not provided manually
                'timeout': 600,
                'cv': 5,
                'models': ['logistic_regression', 'random_forest', 'xgboost', 'lightgbm', 'extra_trees'] # Default selection
            },
            'test': {
                'n_trials': 1, # Minimal for speed
                'timeout': 30, # 30 seconds max
                'cv': 2, # Minimal folds
                # Removed MLP because it is slow
                'models': ['logistic_regression', 'decision_tree'] 
            }
        }

    def _get_default_model(self, name, random_state=42):
        """Returns a default instance of a model without Optuna optimization."""
        # Common models for ensembles
        if self.task_type == 'classification':
            if name == 'logistic_regression': return LogisticRegression(max_iter=1000, random_state=random_state)
            if name == 'random_forest': return RandomForestClassifier(n_estimators=100, random_state=random_state)
            if name == 'xgboost': return xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=random_state)
            if name == 'lightgbm': return lgb.LGBMClassifier(random_state=random_state)
            if name == 'extra_trees': return ExtraTreesClassifier(n_estimators=100, random_state=random_state)
            if name == 'decision_tree': return DecisionTreeClassifier(random_state=random_state)
            if name == 'svm': return SVC(probability=True, random_state=random_state)
            if name == 'linear_svc': return LinearSVC(max_iter=1000, dual=False, random_state=random_state)
            if name == 'knn': return KNeighborsClassifier()
            if name == 'mlp': return MLPClassifier(max_iter=500, random_state=random_state)
            if name == 'sgd_classifier': return SGDClassifier(loss='modified_huber', random_state=random_state)
            if name == 'passive_aggressive': return PassiveAggressiveClassifier(random_state=random_state)
            if name == 'naive_bayes': return GaussianNB()
            if name == 'ridge_classifier': return RidgeClassifier(random_state=random_state)
            if name == 'adaboost': return AdaBoostClassifier(random_state=random_state)
            if name == 'catboost' and CATBOOST_AVAILABLE: return cb.CatBoostClassifier(verbose=0, thread_count=-1, random_seed=random_state)
        elif self.task_type == 'regression':
            if name == 'linear_regression': return LinearRegression()
            if name == 'random_forest': return RandomForestRegressor(n_estimators=100, random_state=random_state)
            if name == 'xgboost': return xgb.XGBRegressor(random_state=random_state)
            if name == 'lightgbm': return lgb.LGBMRegressor(random_state=random_state)
            if name == 'extra_trees': return ExtraTreesRegressor(n_estimators=100, random_state=random_state)
            if name == 'decision_tree': return DecisionTreeRegressor(random_state=random_state)
            if name == 'svm': return SVR()
            if name == 'knn': return KNeighborsRegressor()
            if name == 'mlp': return MLPRegressor(max_iter=500, random_state=random_state)
            if name == 'ridge': return Ridge(random_state=random_state)
            if name == 'lasso': return Lasso(random_state=random_state)
            if name == 'elastic_net': return ElasticNet(random_state=random_state)
            if name == 'sgd_regressor': return SGDRegressor(max_iter=1000, random_state=random_state)
            if name == 'adaboost': return AdaBoostRegressor(random_state=random_state)
            if name == 'catboost' and CATBOOST_AVAILABLE: return cb.CatBoostRegressor(verbose=0, thread_count=-1, random_seed=random_state)
            
        return None

    def _resolve_estimators(self, estimators_config, random_state):
        if not estimators_config:
            return []
        
        # If it's a list of strings (names)
        if isinstance(estimators_config, list) and len(estimators_config) > 0 and isinstance(estimators_config[0], str):
            estimators = []
            for name in estimators_config:
                model = self._get_default_model(name, random_state)
                if model is not None:
                    estimators.append((name, model))
            return estimators
        
        # Assume it's already a list of (name, estimator) tuples
        return estimators_config

    def _get_models(self, trial=None, name=None, random_state=None):
        """
        Retorna a lista de nomes dos modelos ou uma inst√¢ncia espec√≠fica com par√¢metros sugeridos.
        """
        # Prioritize custom models if available
        if name and hasattr(self, 'custom_models') and name in self.custom_models:
            from sklearn.base import clone
            try:
                return clone(self.custom_models[name])
            except:
                import copy
                return copy.deepcopy(self.custom_models[name])

        if self.task_type == 'classification':
            models_config = {
                'voting_ensemble': lambda t: VotingClassifier(
                    estimators=[
                        ('pa', PassiveAggressiveClassifier(max_iter=1000, random_state=random_state, C=0.5)),
                        ('lr', LogisticRegression(max_iter=2000, C=10, solver='saga', n_jobs=-1, random_state=random_state)),
                        ('sgd', SGDClassifier(loss='modified_huber', max_iter=2000, n_jobs=-1, random_state=random_state))
                    ],
                    voting='hard',
                    n_jobs=-1 # Changed to 1 to avoid Windows multiprocessing issues
                ),
                'custom_voting': lambda t: VotingClassifier(
                    estimators=self._resolve_estimators(
                        self.ensemble_config.get('voting_estimators', [
                            ('lr', LogisticRegression(random_state=random_state)), 
                            ('rf', RandomForestClassifier(random_state=random_state))
                        ]),
                        random_state
                    ),
                    voting=self.ensemble_config.get('voting_type', 'soft'),
                    weights=self.ensemble_config.get('voting_weights', None),
                    n_jobs=-1
                ),
                'custom_stacking': lambda t: StackingClassifier(
                    estimators=self._resolve_estimators(
                        self.ensemble_config.get('stacking_estimators', [
                            ('rf', RandomForestClassifier(random_state=random_state)),
                            ('svm', SVC(probability=True, random_state=random_state))
                        ]),
                        random_state
                    ),
                    final_estimator=self._get_default_model(self.ensemble_config.get('stacking_final_estimator'), random_state) 
                                    if isinstance(self.ensemble_config.get('stacking_final_estimator'), str) 
                                    else self.ensemble_config.get('stacking_final_estimator', LogisticRegression(random_state=random_state)),
                    n_jobs=-1
                ),
                'logistic_regression': lambda t: LogisticRegression(
                    C=t.suggest_float('lr_C', 0.001, 100.0, log=True),
                    solver=t.suggest_categorical('lr_solver', ['lbfgs', 'liblinear', 'saga']),
                    max_iter=1000,
                    n_jobs=-1,
                    random_state=random_state
                ),
                'random_forest': lambda t: RandomForestClassifier(
                    n_estimators=t.suggest_int('rf_n_estimators', 100, 1000),
                    max_depth=t.suggest_int('rf_max_depth', 5, 100),
                    min_samples_split=t.suggest_int('rf_min_samples_split', 2, 20),
                    n_jobs=-1,
                    random_state=random_state
                ),
                'xgboost': lambda t: xgb.XGBClassifier(
                    n_estimators=t.suggest_int('xgb_n_estimators', 100, 2000),
                    learning_rate=t.suggest_float('xgb_lr', 0.0001, 0.5, log=True),
                    max_depth=t.suggest_int('xgb_max_depth', 3, 18),
                    subsample=t.suggest_float('xgb_subsample', 0.4, 1.0),
                    colsample_bytree=t.suggest_float('xgb_colsample', 0.4, 1.0),
                    use_label_encoder=False,
                    eval_metric='logloss',
                    n_jobs=-1,
                    random_state=random_state
                ),
                'lightgbm': lambda t: lgb.LGBMClassifier(
                    n_estimators=t.suggest_int('lgb_n_estimators', 100, 2000),
                    learning_rate=t.suggest_float('lgb_lr', 0.0001, 0.5, log=True),
                    num_leaves=t.suggest_int('lgb_leaves', 15, 255),
                    feature_fraction=t.suggest_float('lgb_feature_frac', 0.4, 1.0),
                    bagging_fraction=t.suggest_float('lgb_bagging_frac', 0.4, 1.0),
                    bagging_freq=t.suggest_int('lgb_bagging_freq', 1, 7),
                    verbosity=-1,
                    n_jobs=-1,
                    random_state=random_state
                ),
                'extra_trees': lambda t: ExtraTreesClassifier(
                    n_estimators=t.suggest_int('et_n_estimators', 100, 1000),
                    max_depth=t.suggest_int('et_max_depth', 5, 100),
                    n_jobs=-1,
                    random_state=random_state
                ),
                'adaboost': lambda t: AdaBoostClassifier(
                    n_estimators=t.suggest_int('ada_n_estimators', 50, 500),
                    learning_rate=t.suggest_float('ada_lr', 0.001, 2.0, log=True),
                    random_state=random_state
                ),
                'decision_tree': lambda t: DecisionTreeClassifier(
                    max_depth=t.suggest_int('dt_max_depth', 3, 50),
                    min_samples_split=t.suggest_int('dt_min_samples_split', 2, 20),
                    random_state=random_state
                ),
                'svm': lambda t: SVC(
                    C=t.suggest_float('svm_C', 0.001, 1000.0, log=True),
                    kernel=t.suggest_categorical('svm_kernel', ['linear', 'rbf', 'poly', 'sigmoid']),
                    gamma=t.suggest_categorical('svm_gamma', ['scale', 'auto']),
                    probability=False, 
                    cache_size=2000,
                    max_iter=1000,
                    random_state=random_state
                ),
                'linear_svc': lambda t: LinearSVC(
                    C=t.suggest_float('lsvc_C', 0.01, 100.0, log=True),
                    max_iter=1000, 
                    dual=False, 
                    random_state=random_state
                ),
                'knn': lambda t: KNeighborsClassifier(
                    n_neighbors=t.suggest_int('knn_neighbors', 1, 31),
                    weights=t.suggest_categorical('knn_weights', ['uniform', 'distance']),
                    metric=t.suggest_categorical('knn_metric', ['euclidean', 'manhattan', 'minkowski']),
                    n_jobs=-1
                ),
                'naive_bayes': lambda t: GaussianNB(
                    var_smoothing=t.suggest_float('nb_smoothing', 1e-10, 1e-8, log=True)
                ),
                'ridge_classifier': lambda t: RidgeClassifier(
                    alpha=t.suggest_float('ridge_alpha', 0.01, 10.0, log=True),
                    random_state=random_state
                ),
                'sgd_classifier': lambda t: SGDClassifier(
                    loss=t.suggest_categorical('sgd_loss', ['hinge', 'modified_huber']),
                    penalty=t.suggest_categorical('sgd_penalty', ['l2', 'l1', 'elasticnet']),
                    alpha=t.suggest_float('sgd_alpha', 1e-4, 1e-2, log=True),
                    max_iter=2000, 
                    random_state=random_state,
                    n_jobs=-1
                ),
                'passive_aggressive': lambda t: PassiveAggressiveClassifier(
                    C=t.suggest_float('pa_C', 0.001, 10.0, log=True),
                    fit_intercept=t.suggest_categorical('pa_fit_intercept', [True, False]),
                    max_iter=1000,
                    random_state=random_state,
                    n_jobs=-1
                ),
                'mlp': lambda t: MLPClassifier(
                    hidden_layer_sizes=eval(t.suggest_categorical('mlp_layers', ["(50,)", "(100,)", "(100, 50)", "(100, 100)", "(50, 50, 50)", "(256, 128, 64)"])),
                    activation=t.suggest_categorical('mlp_activation', ['relu', 'tanh', 'logistic']),
                    alpha=t.suggest_float('mlp_alpha', 1e-6, 1e-1, log=True),
                    learning_rate_init=t.suggest_float('mlp_lr', 1e-5, 1e-1, log=True),
                    max_iter=1000,
                    early_stopping=True,
                    random_state=random_state
                ),
                'catboost': lambda t: cb.CatBoostClassifier(
                    iterations=t.suggest_int('cb_iterations', 100, 1000) if self.preset == 'best_quality' else t.suggest_int('cb_iterations', 50, 150),
                    learning_rate=t.suggest_float('cb_lr', 0.001, 0.3, log=True),
                    depth=t.suggest_int('cb_depth', 4, 10) if self.preset == 'best_quality' else t.suggest_int('cb_depth', 4, 6),
                    l2_leaf_reg=t.suggest_float('cb_l2', 1, 10),
                    verbose=0,
                    thread_count=-1,
                    random_seed=random_state
                ) if CATBOOST_AVAILABLE else None,
                'bert-base-uncased': lambda t: TransformersWrapper(model_name='bert-base-uncased', task='classification', learning_rate=t.suggest_float('learning_rate', 1e-6, 1e-3, log=True), epochs=t.suggest_int('num_train_epochs', 1, 20)) if TRANSFORMERS_AVAILABLE else None,
                'distilbert-base-uncased': lambda t: TransformersWrapper(model_name='distilbert-base-uncased', task='classification', learning_rate=t.suggest_float('learning_rate', 1e-6, 1e-3, log=True), epochs=t.suggest_int('num_train_epochs', 1, 20)) if TRANSFORMERS_AVAILABLE else None,
                'roberta-base': lambda t: TransformersWrapper(model_name='roberta-base', task='classification', learning_rate=t.suggest_float('learning_rate', 1e-6, 1e-3, log=True), epochs=t.suggest_int('num_train_epochs', 1, 20)) if TRANSFORMERS_AVAILABLE else None,
                'albert-base-v2': lambda t: TransformersWrapper(model_name='albert-base-v2', task='classification', learning_rate=t.suggest_float('learning_rate', 1e-6, 1e-3, log=True), epochs=t.suggest_int('num_train_epochs', 1, 20)) if TRANSFORMERS_AVAILABLE else None,
                'xlnet-base-cased': lambda t: TransformersWrapper(model_name='xlnet-base-cased', task='classification', learning_rate=t.suggest_float('learning_rate', 1e-6, 1e-3, log=True), epochs=t.suggest_int('num_train_epochs', 1, 20)) if TRANSFORMERS_AVAILABLE else None,
                'microsoft/deberta-v3-base': lambda t: TransformersWrapper(model_name='microsoft/deberta-v3-base', task='classification', learning_rate=t.suggest_float('learning_rate', 1e-6, 1e-3, log=True), epochs=t.suggest_int('num_train_epochs', 1, 20)) if TRANSFORMERS_AVAILABLE else None
            }
        elif self.task_type == 'regression':
            models_config = {
                'custom_voting': lambda t: VotingRegressor(
                    estimators=self._resolve_estimators(
                        self.ensemble_config.get('voting_estimators', [
                            ('lr', LinearRegression()), 
                            ('rf', RandomForestRegressor(random_state=random_state))
                        ]),
                        random_state
                    ),
                    weights=self.ensemble_config.get('voting_weights', None),
                    n_jobs=-1
                ),
                'custom_stacking': lambda t: StackingRegressor(
                    estimators=self._resolve_estimators(
                        self.ensemble_config.get('stacking_estimators', [
                            ('rf', RandomForestRegressor(random_state=random_state)),
                            ('svm', SVR())
                        ]),
                        random_state
                    ),
                    final_estimator=self._get_default_model(self.ensemble_config.get('stacking_final_estimator'), random_state)
                                    if isinstance(self.ensemble_config.get('stacking_final_estimator'), str)
                                    else self.ensemble_config.get('stacking_final_estimator', LinearRegression()),
                    n_jobs=-1
                ),
                'linear_regression': lambda t: LinearRegression(),
                'random_forest': lambda t: RandomForestRegressor(
                    n_estimators=t.suggest_int('rf_n_estimators', 50, 200),
                    max_depth=t.suggest_int('rf_max_depth', 3, 20),
                    random_state=random_state
                ),
                'xgboost': lambda t: xgb.XGBRegressor(
                    n_estimators=t.suggest_int('xgb_n_estimators', 50, 200),
                    learning_rate=t.suggest_float('xgb_lr', 0.01, 0.3),
                    random_state=random_state
                ),
                'lightgbm': lambda t: lgb.LGBMRegressor(
                    n_estimators=t.suggest_int('lgb_n_estimators', 50, 200),
                    learning_rate=t.suggest_float('lgb_lr', 0.01, 0.3),
                    verbosity=-1,
                    random_state=random_state
                ),
                'extra_trees': lambda t: ExtraTreesRegressor(
                    n_estimators=t.suggest_int('et_n_estimators', 50, 200),
                    max_depth=t.suggest_int('et_max_depth', 3, 20),
                    random_state=random_state
                ),
                'adaboost': lambda t: AdaBoostRegressor(
                    n_estimators=t.suggest_int('ada_n_estimators', 50, 200),
                    learning_rate=t.suggest_float('ada_lr', 0.01, 1.0),
                    random_state=random_state
                ),
                'decision_tree': lambda t: DecisionTreeRegressor(
                    max_depth=t.suggest_int('dt_max_depth', 3, 20),
                    random_state=random_state
                ),
                'svm': lambda t: SVR(
                    C=t.suggest_float('svr_C', 0.1, 10.0, log=True),
                    kernel=t.suggest_categorical('svr_kernel', ['linear', 'rbf']),
                    cache_size=1000,
                    max_iter=1000
                ),
                'knn': lambda t: KNeighborsRegressor(
                    n_neighbors=t.suggest_int('knn_neighbors', 3, 15),
                    n_jobs=-1
                ),
                'ridge': lambda t: Ridge(
                    alpha=t.suggest_float('ridge_alpha', 0.1, 10.0),
                    random_state=random_state
                ),
                'lasso': lambda t: Lasso(
                    alpha=t.suggest_float('lasso_alpha', 0.01, 1.0),
                    random_state=random_state
                ),
                'elastic_net': lambda t: ElasticNet(
                    alpha=t.suggest_float('en_alpha', 0.01, 1.0),
                    l1_ratio=t.suggest_float('en_l1_ratio', 0.1, 0.9),
                    random_state=random_state
                ),
                'sgd_regressor': lambda t: SGDRegressor(max_iter=1000, random_state=random_state),
                'mlp': lambda t: MLPRegressor(
                    hidden_layer_sizes=t.suggest_categorical('mlp_hidden', [(50,), (100,), (50, 50)]),
                    activation=t.suggest_categorical('mlp_activation', ['relu', 'tanh']),
                    alpha=t.suggest_float('mlp_alpha', 1e-4, 1e-1, log=True),
                    max_iter=t.suggest_categorical('max_iter', [200, 500]) if not t.user_attrs.get("max_iter") else t.user_attrs.get("max_iter"),
                    random_state=random_state,
                    early_stopping=True # Force early stopping for speed
                ),
                'catboost': lambda t: cb.CatBoostRegressor(
                    iterations=t.suggest_int('cb_iterations', 100, 1000) if self.preset == 'best_quality' else t.suggest_int('cb_iterations', 50, 150),
                    learning_rate=t.suggest_float('cb_lr', 0.001, 0.3, log=True),
                    depth=t.suggest_int('cb_depth', 4, 10) if self.preset == 'best_quality' else t.suggest_int('cb_depth', 4, 6),
                    l2_leaf_reg=t.suggest_float('cb_l2', 1, 10),
                    verbose=0,
                    thread_count=-1,
                    random_seed=random_state
                ) if CATBOOST_AVAILABLE else None,
                'bert-base-uncased-reg': lambda t: TransformersWrapper(model_name='bert-base-uncased', task='regression', learning_rate=t.suggest_float('learning_rate', 1e-6, 1e-3, log=True), epochs=t.suggest_int('num_train_epochs', 1, 20)) if TRANSFORMERS_AVAILABLE else None,
                'distilbert-base-uncased-reg': lambda t: TransformersWrapper(model_name='distilbert-base-uncased', task='regression', learning_rate=t.suggest_float('learning_rate', 1e-6, 1e-3, log=True), epochs=t.suggest_int('num_train_epochs', 1, 20)) if TRANSFORMERS_AVAILABLE else None,
                'roberta-base-reg': lambda t: TransformersWrapper(model_name='roberta-base', task='regression', learning_rate=t.suggest_float('learning_rate', 1e-6, 1e-3, log=True), epochs=t.suggest_int('num_train_epochs', 1, 20)) if TRANSFORMERS_AVAILABLE else None,
                'microsoft/deberta-v3-small': lambda t: TransformersWrapper(model_name='microsoft/deberta-v3-small', task='regression', learning_rate=t.suggest_float('learning_rate', 1e-6, 1e-3, log=True), epochs=t.suggest_int('num_train_epochs', 1, 20)) if TRANSFORMERS_AVAILABLE else None
            }
        elif self.task_type == 'clustering':
            models_config = {
                'kmeans': lambda t: KMeans(
                    n_clusters=t.suggest_int('km_n_clusters', 2, 12),
                    n_init=10
                ),
                'agglomerative': lambda t: AgglomerativeClustering(
                    n_clusters=t.suggest_int('agg_n_clusters', 2, 12)
                ),
                'dbscan': lambda t: DBSCAN(
                    eps=t.suggest_float('db_eps', 0.1, 2.0),
                    min_samples=t.suggest_int('db_min_samples', 2, 10),
                    n_jobs=None
                ),
                'gaussian_mixture': lambda t: GaussianMixture(
                    n_components=t.suggest_int('gm_n_components', 2, 12)
                ),
                'mean_shift': lambda t: MeanShift(n_jobs=None),
                'birch': lambda t: Birch(
                    n_clusters=t.suggest_int('birch_n_clusters', 2, 12)
                ),
                'spectral': lambda t: SpectralClustering(
                    n_clusters=t.suggest_int('spectral_n_clusters', 2, 12),
                    n_jobs=None
                )
            }
        elif self.task_type == 'time_series':
            models_config = {
                'random_forest_ts': lambda t: RandomForestRegressor(
                    n_estimators=t.suggest_int('rf_ts_n_estimators', 50, 200),
                    n_jobs=None
                ),
                'xgboost_ts': lambda t: xgb.XGBRegressor(
                    n_estimators=t.suggest_int('xgb_ts_n_estimators', 50, 200),
                    n_jobs=None
                ),
                'extra_trees_ts': lambda t: ExtraTreesRegressor(
                    n_estimators=t.suggest_int('et_ts_n_estimators', 50, 200),
                    n_jobs=None
                ),
                'catboost_ts': lambda t: cb.CatBoostRegressor(
                    iterations=t.suggest_int('cb_ts_iterations', 50, 200),
                    verbose=0,
                    thread_count=-1
                ) if CATBOOST_AVAILABLE else None
            }
        elif self.task_type == 'anomaly_detection':
            models_config = {
                'isolation_forest': lambda t: IsolationForest(
                    n_estimators=t.suggest_int('if_n_estimators', 50, 200),
                    contamination=t.suggest_float('if_contamination', 0.01, 0.2),
                    random_state=42,
                    n_jobs=None
                ),
                'local_outlier_factor': lambda t: LocalOutlierFactor(
                    n_neighbors=t.suggest_int('lof_neighbors', 10, 50),
                    contamination=t.suggest_float('lof_contamination', 0.01, 0.2),
                    novelty=True,
                    n_jobs=None
                ),
                'elliptic_envelope': lambda t: EllipticEnvelope(
                    contamination=t.suggest_float('ee_contamination', 0.01, 0.2),
                    random_state=42
                ),
                'one_class_svm': lambda t: OneClassSVM(
                    nu=t.suggest_float('oc_nu', 0.01, 0.2),
                    kernel=t.suggest_categorical('oc_kernel', ['rbf', 'poly', 'sigmoid'])
                )
            }
        else:
            return {}

        if name is None:
            # Filtra modelos que retornam None (ex: bibliotecas n√£o instaladas)
            available = []
            for k, v in models_config.items():
                try:
                    if v(None) is not None:
                        available.append(k)
                except:
                    # Se a lambda falhar com None (ex: t.suggest_int), assumimos que o modelo est√° dispon√≠vel
                    # pois o erro vem da l√≥gica do trial, n√£o da aus√™ncia da biblioteca
                    available.append(k)
            
            if hasattr(self, 'custom_models') and self.custom_models:
                available.extend(list(self.custom_models.keys()))
            return available
        
        if name in models_config:
            # Passamos o trial para a lambda para instanciar o modelo com os par√¢metros sugeridos
            return models_config[name](trial)
        
        return None

    def get_available_models(self):
        """Returns a list of available model names for the current task type."""
        return self._get_models()

    def train(self, X_train, y_train=None, n_trials=None, timeout=None, callback=None, selected_models=None, early_stopping_rounds=None, experiment_name="AutoML_Experiment", manual_params=None, random_state=42, validation_strategy='cv', validation_params=None, custom_models=None, X_raw=None, time_budget=None, optimization_mode='bayesian', optimization_metric='accuracy', **kwargs):
        # Usar configura√ß√µes do preset se n_trials/timeout n√£o forem fornecidos
        preset_config = self.preset_configs.get(self.preset, self.preset_configs['medium'])
        n_trials = n_trials if n_trials is not None else preset_config['n_trials']
        timeout = timeout if timeout is not None else preset_config.get('timeout')
        
        # Armazenar modelos customizados (uploaded ou registrados)
        self.custom_models = custom_models if custom_models else {}
        
        # Se selected_models n√£o for fornecido, usa a lista do preset
        if selected_models is None:
            selected_models = preset_config['models']
            
        self.ts_metadata = kwargs if self.task_type == 'time_series' else {}
        self.random_state = random_state
        self.ensemble_config = kwargs.get('ensemble_config', {})
        
        global_start_time = time.time()

        
        # Compatibilidade com par√¢metro antigo auto_split
        if kwargs.get('auto_split', False):
            validation_strategy = 'auto_split'
            
        # L√≥gica de Valida√ß√£o Autom√°tica
        if validation_strategy == 'auto':
            if self.task_type == 'time_series':
                validation_strategy = 'time_series_cv'
                logger.info("ü§ñ Valida√ß√£o Autom√°tica: Escolhido TimeSeriesSplit (dado que √© s√©rie temporal).")
            else:
                # Se tivermos dados suficientes, holdout √© mais r√°pido. Se poucos, CV √© mais robusto.
                if hasattr(X_train, 'shape'):
                    n_samples = X_train.shape[0]
                else:
                    n_samples = len(X_train)

                if n_samples < 1000:
                    validation_strategy = 'cv'
                    logger.info(f"ü§ñ Valida√ß√£o Autom√°tica: Escolhido Cross-Validation (N={n_samples} < 1000).")
                else:
                    validation_strategy = 'holdout'
                    logger.info(f"ü§ñ Valida√ß√£o Autom√°tica: Escolhido Holdout/Train-Test Split (N={n_samples} >= 1000).")

        if validation_params is None:
            validation_params = {}
        
        tracker = None
        # Define DummyTracker here to ensure scope availability
        class DummyTracker:
            def log_experiment(self, **kwargs):
                return "dummy_run_id"

        try:
            from mlops_utils import MLFlowTracker
            tracker = MLFlowTracker(experiment_name)
            logger.info(f"‚úÖ MLFlowTracker initialized for experiment: {experiment_name}")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è MLFlowTracker init failed: {e}. Proceeding without MLflow logging.")
            tracker = DummyTracker()

        # Early Stopping & Summary Logic
        best_score_so_far = -np.inf
        trials_without_improvement = 0
        model_trial_counts = {} # Corrigir √≠ndice do gr√°fico: contador real por modelo
        self.model_summaries = {} # Armazenar melhor m√©trica de cada modelo
        self.detailed_model_reports = {} # Armazenar relat√≥rios detalhados para exibi√ß√£o

        def objective(trial, forced_model=None):
            nonlocal best_score_so_far, trials_without_improvement
            
            # Global Time Budget Check
            if time_budget is not None:
                elapsed = time.time() - global_start_time
                if elapsed > time_budget:
                    logger.warning(f"‚è∞ Global time budget exceeded ({elapsed:.2f}s > {time_budget}s). Stopping study.")
                    trial.study.stop()
                    return 0.0

            # 1. Determinar o modelo a ser usado
            all_available = selected_models if selected_models else self.get_available_models()
            if not all_available: all_available = self.get_available_models()

            if forced_model:
                # Force specific model without changing search space (avoiding dynamic space error)
                model_name = forced_model
                trial.set_user_attr("model_name", model_name)
            else:
                model_name = trial.suggest_categorical('model_name', all_available)

            # Identificador √∫nico para este trial de modelo
            model_trial_counts[model_name] = model_trial_counts.get(model_name, 0) + 1
            trial_num_for_model = model_trial_counts[model_name]
            full_trial_name = f"{model_name} - Trial {trial_num_for_model}"
            
            # Determinar a seed espec√≠fica para este modelo
            current_seed = self.random_state
            if isinstance(self.random_state, dict):
                current_seed = self.random_state.get(model_name, 42)
            
            logger.info(f"üìç Trial {trial.number} mapeado para {full_trial_name} (Seed: {current_seed})")

            run_id = None

            # Early Stopping Global
            min_improvement = 0.0001
            if early_stopping_rounds and trials_without_improvement >= early_stopping_rounds:
                trial.study.stop()
                return 0

            # 2. Instanciar o modelo espec√≠fico sugerido para este trial (Lazy Loading)
            model = self._get_models(trial=trial, name=model_name, random_state=current_seed)
            
            if model is None:
                return -1.0
            
            # Determine which input data to use (Vectorized or Raw)
            effective_X = X_train
            if X_raw is not None and isinstance(model, TransformersWrapper):
                effective_X = X_raw
                logger.info(f"ü§ñ Model {model_name} uses Transformers: Using RAW TEXT input.")
            elif isinstance(model, TransformersWrapper):
                logger.warning(f"‚ö†Ô∏è Model {model_name} is a Transformer but X_raw was not provided. Expect failure if input is vectorized.")

            # L√≥gica de Valida√ß√£o e Split de Dados
            X_tr, X_val, y_tr, y_val = None, None, None, None
            
            # Apenas para m√©todos que usam holdout expl√≠cito (auto_split ou holdout manual)
            use_explicit_validation = validation_strategy in ['auto_split', 'holdout']
            
            if use_explicit_validation and self.task_type in ['classification', 'regression', 'time_series']:
                if validation_strategy == 'auto_split':
                    split_ratio = trial.suggest_float('data_split_ratio', 0.6, 0.9)
                else: # holdout
                    test_size = validation_params.get('test_size', 0.2)
                    split_ratio = 1.0 - test_size
                
                # Cache key should include if we are using raw or vectorized data
                is_raw = (effective_X is X_raw)
                if not hasattr(self, '_split_cache'): self._split_cache = {}
                cache_key = f"{split_ratio}_{self.task_type}_{current_seed}_{validation_strategy}_{is_raw}"
                
                if cache_key in self._split_cache:
                    X_tr, X_val, y_tr, y_val = self._split_cache[cache_key]
                else:
                    if self.task_type == 'time_series':
                        split_idx = int(len(effective_X) * split_ratio)
                        if isinstance(effective_X, pd.DataFrame):
                            X_tr, X_val = effective_X.iloc[:split_idx], effective_X.iloc[split_idx:]
                            y_tr, y_val = y_train.iloc[:split_idx], y_train.iloc[split_idx:]
                        else:
                            X_tr, X_val = effective_X[:split_idx], effective_X[split_idx:]
                            y_tr, y_val = y_train[:split_idx], y_train[split_idx:]
                    else:
                        if y_train is not None:
                            X_tr, X_val, y_tr, y_val = train_test_split(effective_X, y_train, train_size=split_ratio, random_state=current_seed)
                        else:
                            X_tr, X_val = train_test_split(effective_X, train_size=split_ratio, random_state=current_seed)
                            y_tr, y_val = None, None
                    if len(self._split_cache) > 10: self._split_cache.clear()
                    self._split_cache[cache_key] = (X_tr, X_val, y_tr, y_val)
            else:
                # Para CV, usamos os dados completos no cross_val_score
                X_tr, y_tr = effective_X, y_train
                X_val, y_val = None, None

            start_time = time.time()
            logger.info(f"‚è≥ Treinando {full_trial_name}...")
            trial_metrics = {}
            trial_params = trial.params.copy()
            trial_params['task_type'] = self.task_type
            
            try:
                if self.task_type in ['classification', 'regression', 'time_series']:
                    if use_explicit_validation:
                        logger.info(f"DEBUG: fit() called. X_tr type: {type(X_tr)}, shape: {X_tr.shape if hasattr(X_tr, 'shape') else len(X_tr)}")
                        logger.info(f"DEBUG: y_tr type: {type(y_tr)}, shape: {y_tr.shape if hasattr(y_tr, 'shape') else len(y_tr)}")
                        logger.info(f"DEBUG: Model: {model}")
                        model.fit(X_tr, y_tr)
                        logger.info("DEBUG: fit() completed.")
                        y_pred_val = model.predict(X_val)
                        if self.task_type == 'classification':
                            if optimization_metric == 'accuracy':
                                score = accuracy_score(y_val, y_pred_val)
                            elif optimization_metric == 'f1':
                                score = f1_score(y_val, y_pred_val, average='weighted')
                            elif optimization_metric == 'precision':
                                score = precision_score(y_val, y_pred_val, average='weighted')
                            elif optimization_metric == 'recall':
                                score = recall_score(y_val, y_pred_val, average='weighted')
                            elif optimization_metric == 'roc_auc':
                                try:
                                    y_prob_val = model.predict_proba(X_val)
                                    score = roc_auc_score(y_val, y_prob_val, multi_class='ovr')
                                except:
                                    score = 0.5 # Fail-safe
                            else:
                                score = accuracy_score(y_val, y_pred_val)

                            trial_metrics['accuracy'] = accuracy_score(y_val, y_pred_val)
                            trial_metrics['score'] = score
                        else:
                            if optimization_metric == 'r2':
                                score = r2_score(y_val, y_pred_val)
                            elif optimization_metric == 'rmse':
                                score = -np.sqrt(mean_squared_error(y_val, y_pred_val)) # Negativo pois Optuna maximiza
                            elif optimization_metric == 'mae':
                                score = -mean_absolute_error(y_val, y_pred_val)
                            else:
                                score = r2_score(y_val, y_pred_val)
                            
                            trial_metrics['r2'] = r2_score(y_val, y_pred_val)
                            trial_metrics['score'] = score
                    else:
                        # Cross Validation Logic
                        n_splits = validation_params.get('folds', 3) if validation_params else 3
                        # Fallback se folds n√£o estiver definido
                        if not isinstance(n_splits, int): n_splits = 3
                        
                        if self.task_type == 'time_series' or validation_strategy == 'time_series_cv':
                            cv = TimeSeriesSplit(n_splits=n_splits)
                            scoring = 'neg_root_mean_squared_error' if optimization_metric == 'rmse' else 'r2'
                        elif self.task_type == 'classification':
                            if optimization_metric == 'roc_auc':
                                scoring = 'roc_auc_ovr'
                            elif optimization_metric == 'f1':
                                scoring = 'f1_weighted'
                            elif optimization_metric == 'precision':
                                scoring = 'precision_weighted'
                            elif optimization_metric == 'recall':
                                scoring = 'recall_weighted'
                            else:
                                scoring = 'accuracy'
                                
                            if validation_strategy == 'stratified_cv':
                                cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=current_seed)
                            else:
                                cv = KFold(n_splits=n_splits, shuffle=True, random_state=current_seed)
                        else: # Regression
                            if optimization_metric == 'rmse':
                                scoring = 'neg_root_mean_squared_error'
                            elif optimization_metric == 'mae':
                                scoring = 'neg_mean_absolute_error'
                            else:
                                scoring = 'r2'
                                
                            cv = KFold(n_splits=n_splits, shuffle=True, random_state=current_seed)
                            
                        score = cross_val_score(model, X_tr, y_tr, cv=cv, scoring=scoring).mean()
                        trial_metrics[scoring] = score
                        trial_metrics['score'] = score
                        
                        # Para salvar o modelo e artefatos, precisamos dar fit no treino completo do trial
                        # Nota: Fit final no trial_set sem CV para logging
                        logger.info(f"‚ú® Finalizando treino do modelo {full_trial_name}...")
                        model.fit(X_tr, y_tr)
                        logger.info(f"‚úÖ Treino finalizado para {full_trial_name}")

                        # ADDED: Log multiple metrics for every trial (not just the optimization one)
                        if self.task_type == 'classification' and hasattr(model, 'predict'):
                            try:
                                # Get predictions on a holdout subset or OOF if possible (OOF is expensive here, so use simple predict on train for logging purposes - warning: overfitting metrics!)
                                # Better: Use a small holdout from X_tr for quick logging metrics
                                X_sub_train, X_sub_val, y_sub_train, y_sub_val = train_test_split(X_tr, y_tr, test_size=0.2, random_state=42)
                                model.fit(X_sub_train, y_sub_train)
                                y_sub_pred = model.predict(X_sub_val)
                                trial_metrics['val_accuracy'] = accuracy_score(y_sub_val, y_sub_pred)
                                trial_metrics['val_f1'] = f1_score(y_sub_val, y_sub_pred, average='weighted')
                                # Re-fit full for the final model artifact
                                model.fit(X_tr, y_tr)
                            except: pass
                        
                elif self.task_type == 'anomaly_detection':
                    model.fit(X_tr)
                    if hasattr(model, 'decision_function'):
                        score = model.decision_function(X_tr).mean()
                    else:
                        score = 0
                    trial_metrics['decision_score'] = score
                else: # clustering
                    model.fit(X_tr)
                    labels = model.labels_ if hasattr(model, 'labels_') else model.predict(X_tr)
                    if len(set(labels)) > 1:
                        score = silhouette_score(X_tr, labels)
                    else:
                        score = -1
                    trial_metrics['silhouette'] = score

                # Unified Logging for all tasks/strategies
                logger.info(f"üìä Registrando {full_trial_name} no MLflow...")
                run_id = tracker.log_experiment(
                    params=trial_params,
                    metrics=trial_metrics,
                    model=model,
                    model_name=full_trial_name,
                    register=False
                )
                logger.info(f"‚úî {full_trial_name} registrado com ID: {run_id}")

            except Exception as e:
                logger.error(f"Error during trial for {model_name}: {e}")
                score = -1.0
                trial_metrics['error'] = 1.0

            duration = time.time() - start_time
            trial_metrics['duration'] = duration
            
            # Enrich trial metrics with parameters for easy access in callbacks
            # Prefix params to distinguish them
            for p_k, p_v in trial_params.items():
                if p_k not in trial_metrics:
                    trial_metrics[f"param_{p_k}"] = p_v
            
            trial.set_user_attr("run_id", run_id)
            trial.set_user_attr("full_name", full_trial_name)

            # Atualizar resumo do modelo (melhor trial de cada algoritmo)
            if model_name not in self.model_summaries or score > self.model_summaries[model_name]['score']:
                self.model_summaries[model_name] = {
                    'score': score,
                    'metrics': trial_metrics,
                    'params': trial_params,
                    'duration': duration,
                    'trial_name': full_trial_name
                }

            if callback:
                callback(trial, score, full_trial_name, duration, trial_metrics)
                
            if score > (best_score_so_far + min_improvement):
                best_score_so_far = score
                trials_without_improvement = 0
            else:
                trials_without_improvement += 1

            return score

        # All our metrics are better when larger
        direction = 'maximize'
        
        # Determinar seed inicial para o sampler
        sampler_seed = self.random_state if isinstance(self.random_state, int) else 42
        
        # Sele√ß√£o de Sampler e Pruner baseada no optimization_mode
        sampler = None
        pruner = None
        
        if optimization_mode == 'random':
            sampler = optuna.samplers.RandomSampler(seed=sampler_seed)
            logger.info("üé≤ Modo de Otimiza√ß√£o: Random Search")
        elif optimization_mode == 'grid':
            # Nota: GridSampler requer espa√ßo de busca definido a priori, o que n√£o temos aqui.
            # Fallback para RandomSampler (que faz busca estoc√°stica, similar a Grid n√£o-exaustivo)
            sampler = optuna.samplers.RandomSampler(seed=sampler_seed)
            logger.warning("‚ö†Ô∏è Grid Search selecionado, mas usando Random Search devido √† defini√ß√£o din√¢mica do espa√ßo de busca.")
        elif optimization_mode == 'hyperband':
            # Hyperband usa TPE + Pruning agressivo
            sampler = optuna.samplers.TPESampler(n_startup_trials=5, seed=sampler_seed)
            pruner = optuna.pruners.HyperbandPruner(min_resource=1, max_resource=n_trials, reduction_factor=3)
            logger.info("‚ö° Modo de Otimiza√ß√£o: Hyperband (TPE + HyperbandPruner)")
        else: # bayesian (default) or auto
            sampler = optuna.samplers.TPESampler(n_startup_trials=max(n_trials // 3, 5), seed=sampler_seed)
            logger.info("üß† Modo de Otimiza√ß√£o: Bayesian Optimization (TPE)")
        
        study = optuna.create_study(
            direction=direction,
            sampler=sampler,
            pruner=pruner
        )
        
        # Identificar modelos est√°ticos (sem hiperpar√¢metros para otimizar)
        static_models = {
            'naive_bayes', 'ridge_classifier', 
            'linear_regression', 'mean_shift',
            'elliptic_envelope'
        }
        
        models_to_tune = selected_models if selected_models else self.get_available_models()
        
        for m_name in models_to_tune:
            # 1. Verifica√ß√£o do Or√ßamento Global
            if time_budget is not None:
                elapsed_total = time.time() - global_start_time
                if elapsed_total > time_budget:
                    logger.warning(f"‚è∞ Tempo total excedido ({elapsed_total:.2f}s > {time_budget}s). Interrompendo treino de novos modelos.")
                    break
                
                remaining_budget = time_budget - elapsed_total
                
                # 2. Ajuste do Timeout do Modelo Atual
                # Usa o menor valor entre o timeout definido para o modelo e o tempo restante global
                current_timeout = timeout
                if current_timeout is None:
                    current_timeout = remaining_budget
                else:
                    current_timeout = min(current_timeout, remaining_budget)
                    
                # Pula se o tempo restante for irrelevante (< 1s)
                if current_timeout < 1.0:
                    logger.warning(f"‚è∞ Tempo restante insuficiente para {m_name}. Pulando.")
                    continue
            else:
                current_timeout = timeout

            # Se a seed for por modelo, atualizamos o sampler para garantir reprodutibilidade por modelo
            if isinstance(self.random_state, dict):
                model_seed = self.random_state.get(m_name, 42)
                if optimization_mode == 'random' or optimization_mode == 'grid':
                     study.sampler = optuna.samplers.RandomSampler(seed=model_seed)
                else:
                     study.sampler = optuna.samplers.TPESampler(n_startup_trials=min(n_trials, 10), seed=model_seed)
                logger.info(f"üé≤ Sampler seed atualizado para {model_seed} (Modelo: {m_name})")
            # Se houver par√¢metros manuais para este modelo, enfileira uma tentativa com eles
            if manual_params and manual_params.get('model_name') == m_name:
                p = {'model_name': m_name}
                p.update({k: v for k, v in manual_params.items() if k != 'model_name'})
                study.enqueue_trial(p)
                logger.info(f"üíâ Enfileirando tentativa manual para {m_name}")

            # Se o modelo for est√°tico, rodamos apenas 1 vez
            current_n_trials = 1 if m_name in static_models else n_trials
            
            trials_without_improvement = 0 
            logger.info(f"üöÄ Iniciando otimiza√ß√£o para o modelo: {m_name} ({current_n_trials} trials, Timeout: {current_timeout:.2f}s)")
            
            try:
                # O par√¢metro timeout aqui garante que o Optuna pare de iniciar novos trials ap√≥s o tempo limite
                study.optimize(
                    lambda t: objective(t, forced_model=m_name), 
                    n_trials=current_n_trials, 
                    timeout=current_timeout
                )
            except Exception as e:
                logger.error(f"Erro durante otimiza√ß√£o de {m_name}: {e}")
            
            # --- Per-Model Reporting & Logging (Post-Optimization) ---
            try:
                # 1. Recuperar o melhor trial para este modelo espec√≠fico
                best_trial_for_model = None
                best_score_for_model = -np.inf
                
                for t in study.trials:
                    # Check if trial was for this model and completed
                    if t.state == optuna.trial.TrialState.COMPLETE and t.user_attrs.get("model_name") == m_name:
                         if t.value > best_score_for_model:
                             best_score_for_model = t.value
                             best_trial_for_model = t
                
                if best_trial_for_model:
                    logger.info(f"üèÜ Melhor trial para {m_name}: Trial {best_trial_for_model.number} (Score: {best_score_for_model:.4f})")
                    
                    # 2. Re-instanciar o melhor modelo
                    best_params_model = best_trial_for_model.params.copy()
                    best_params_model['model_name'] = m_name # Ensure model name is present
                    
                    # Use specific seed if applicable
                    model_seed = self.random_state
                    if isinstance(self.random_state, dict):
                        model_seed = self.random_state.get(m_name, 42)
                    
                    best_model_instance = self._instantiate_model(m_name, best_params_model)
                    
                    # Set seed
                    if hasattr(best_model_instance, 'random_state'):
                        best_model_instance.set_params(random_state=model_seed)
                    elif hasattr(best_model_instance, 'random_seed'):
                         best_model_instance.set_params(random_seed=model_seed)
                    
                    # Enable probability for classification plots
                    if self.task_type == 'classification' and hasattr(best_model_instance, 'predict_proba'):
                         # SVM needs explicit probability=True
                         if m_name == 'svm' or isinstance(best_model_instance, SVC):
                             best_model_instance.set_params(probability=True)

                    # 3. Gerar previs√µes de valida√ß√£o robustas (CV ou Holdout) para os gr√°ficos
                    # Precisamos de y_true e y_pred (e y_proba)
                    y_true_plot = None
                    y_pred_plot = None
                    y_proba_plot = None
                    
                    # Handle Data Input (Raw vs Vectorized)
                    effective_X_plot = X_train
                    if X_raw is not None and isinstance(best_model_instance, TransformersWrapper):
                        effective_X_plot = X_raw
                    
                    # Generate predictions using cross_val_predict or holdout split
                    # To be consistent with optimization, let's use CV if data allows, otherwise Holdout
                    # For plotting, we want out-of-sample predictions on the training set
                    
                    # Check for high dimensionality to avoid memory explosion in reports
                    is_high_dim = False
                    if hasattr(effective_X_plot, 'shape') and effective_X_plot.shape[1] > 5000:
                        is_high_dim = True
                        logger.warning(f"‚ö†Ô∏è High dimensionality detected ({effective_X_plot.shape[1]} features). Skipping full report generation to save memory.")
                    
                    if validation_strategy == 'time_series_cv' or self.task_type == 'time_series':
                         # Time Series is tricky for 'full' plot. We'll skip complex plot generation here 
                         # or just do a simple validation split at end
                         pass 
                    elif hasattr(effective_X_plot, 'shape') and effective_X_plot.shape[0] < 50:
                         # Too small for CV plotting, skip
                         pass
                    elif not hasattr(effective_X_plot, 'shape') and len(effective_X_plot) < 50:
                         pass
                    elif is_high_dim:
                         # Skip detailed plotting for high dim data
                         pass
                    else:
                        # Use 3-fold CV to get clean predictions for the whole training set
                        # This gives us a confusion matrix for the whole dataset based on OOF predictions
                        try:
                            # IMPORTANT: Ensure X is numeric/processed before CV
                            if isinstance(effective_X_plot, pd.DataFrame):
                                # If DataFrame, check dtypes. If object, might fail if model not pipeline.
                                # But best_model_instance SHOULD handle it if it was trained on it.
                                pass
                                
                            logger.info(f"üìä Gerando previs√µes via CV (3-fold) para relat√≥rio do modelo {m_name}...")
                            method = 'predict'
                            y_pred_plot = cross_val_predict(best_model_instance, effective_X_plot, y_train, cv=3, n_jobs=-1)
                            y_true_plot = y_train
                            
                            if self.task_type == 'classification' and hasattr(best_model_instance, 'predict_proba'):
                                try:
                                    y_proba_plot = cross_val_predict(best_model_instance, effective_X_plot, y_train, cv=3, n_jobs=-1, method='predict_proba')
                                except:
                                    pass
                        except Exception as cv_err:
                            logger.warning(f"Failed to generate CV predictions for report: {cv_err}")
                            # Fallback: Simple predict on X_train (Overfit warning)
                            logger.info("Falling back to training set predictions (Warning: Metrics may be optimistic)")
                            best_model_instance.fit(effective_X_plot, y_train)
                            y_pred_plot = best_model_instance.predict(effective_X_plot)
                            y_true_plot = y_train
                            if self.task_type == 'classification' and hasattr(best_model_instance, 'predict_proba'):
                                y_proba_plot = best_model_instance.predict_proba(effective_X_plot)
                    
                    # 4. Calcular m√©tricas detalhadas
                    report_metrics = {}
                    if y_true_plot is not None and y_pred_plot is not None:
                        if self.task_type == 'classification':
                            report_metrics['accuracy'] = accuracy_score(y_true_plot, y_pred_plot)
                            report_metrics['f1'] = f1_score(y_true_plot, y_pred_plot, average='weighted')
                            report_metrics['precision'] = precision_score(y_true_plot, y_pred_plot, average='weighted')
                            report_metrics['recall'] = recall_score(y_true_plot, y_pred_plot, average='weighted')
                            if y_proba_plot is not None:
                                try:
                                    report_metrics['roc_auc'] = roc_auc_score(y_true_plot, y_proba_plot, multi_class='ovr')
                                except: pass
                        elif self.task_type == 'regression':
                            report_metrics['r2'] = r2_score(y_true_plot, y_pred_plot)
                            report_metrics['rmse'] = np.sqrt(mean_squared_error(y_true_plot, y_pred_plot))
                            report_metrics['mae'] = mean_absolute_error(y_true_plot, y_pred_plot)

                    # 5. Criar objetos de plotagem (Matplotlib Figures)
                    plots = {}
                    if y_true_plot is not None and y_pred_plot is not None:
                        import matplotlib.pyplot as plt
                        import seaborn as sns
                        
                        if self.task_type == 'classification':
                            # Confusion Matrix
                            cm = confusion_matrix(y_true_plot, y_pred_plot)
                            fig_cm, ax_cm = plt.subplots(figsize=(6, 5))
                            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax_cm)
                            ax_cm.set_title(f'Confusion Matrix - {m_name}')
                            ax_cm.set_ylabel('True Label')
                            ax_cm.set_xlabel('Predicted Label')
                            plots[f'confusion_matrix_{m_name}'] = fig_cm
                            
                            if y_proba_plot is not None and hasattr(best_model_instance, 'classes_'):
                                from sklearn.preprocessing import label_binarize
                                from sklearn.metrics import roc_curve, auc
                                
                                # ROC Curve
                                fig_roc, ax_roc = plt.subplots(figsize=(6, 5))
                                n_classes = len(np.unique(y_true_plot))
                                
                                if n_classes == 2:
                                    fpr, tpr, _ = roc_curve(y_true_plot, y_proba_plot[:, 1])
                                    roc_auc = auc(fpr, tpr)
                                    ax_roc.plot(fpr, tpr, label=f'ROC curve (area = {roc_auc:.2f})')
                                else:
                                    # Multiclass ROC (Macro)
                                    # This is a simplification
                                    pass
                                    
                                ax_roc.plot([0, 1], [0, 1], 'k--')
                                ax_roc.set_xlabel('False Positive Rate')
                                ax_roc.set_ylabel('True Positive Rate')
                                ax_roc.set_title(f'ROC Curve - {m_name}')
                                ax_roc.legend(loc="lower right")
                                plots[f'roc_curve_{m_name}'] = fig_roc

                        elif self.task_type == 'regression':
                            # Pred vs Actual
                            fig_reg, ax_reg = plt.subplots(figsize=(6, 5))
                            ax_reg.scatter(y_true_plot, y_pred_plot, alpha=0.5)
                            ax_reg.plot([y_true_plot.min(), y_true_plot.max()], [y_true_plot.min(), y_true_plot.max()], 'k--', lw=2)
                            ax_reg.set_xlabel('Actual')
                            ax_reg.set_ylabel('Predicted')
                            ax_reg.set_title(f'Actual vs Predicted - {m_name}')
                            plots[f'pred_vs_actual_{m_name}'] = fig_reg
                            # plt.close(fig_reg) # Keep open for passing to callback? No, pyplot is stateful.
                            # Better: Return the Figure object.
                            
                    # 6. Salvar no MLflow (sob o run_id do melhor trial)
                    # Precisamos recuperar o run_id do trial
                    best_run_id = best_trial_for_model.user_attrs.get("run_id")
                    if best_run_id and tracker and not isinstance(tracker, DummyTracker):
                        logger.info(f"üíæ Salvando plots adicionais no MLflow Run ID: {best_run_id}")
                        with mlflow.start_run(run_id=best_run_id):
                            # Log full params just in case
                            mlflow.log_params(best_params_model)
                            # Log extra metrics
                            if report_metrics:
                                mlflow.log_metrics({f"val_{k}": v for k, v in report_metrics.items()})
                            
                            # Log plots
                            for plot_name, fig_obj in plots.items():
                                try:
                                    mlflow.log_figure(fig_obj, f"{plot_name}.png")
                                    # plt.close(fig_obj) # DO NOT CLOSE if we want to show in Streamlit
                                except Exception as e:
                                    logger.warning(f"Failed to log figure {plot_name} to MLflow: {e}")
                    
                    # 7. Disparar Callback de Relat√≥rio Completo
                    if callback:
                         # Prepare rich report object
                         full_report = {
                             'model_name': m_name,
                             'best_trial_number': best_trial_for_model.number,
                             'score': best_score_for_model,
                             'params': best_params_model,
                             'metrics': report_metrics,
                             'plots': plots,
                             'run_id': best_run_id
                         }
                         
                         report_payload = trial_metrics.copy() if 'trial_metrics' in locals() else {}
                         report_payload['__report__'] = full_report
                         
                         callback(best_trial_for_model, best_score_for_model, f"{m_name} - FINAL", 0.0, report_payload)

            except Exception as e:
                logger.error(f"‚ùå Erro ao gerar relat√≥rio final para {m_name}: {e}")
                # import traceback
                # traceback.print_exc()

            # Resetar o flag de parada para permitir que o pr√≥ximo modelo seja otimizado
            # O study.stop() define este flag como True
            if hasattr(study, '_stop_flag'):
                study._stop_flag = False
            logger.info(f"‚úÖ Otimiza√ß√£o para {m_name} finalizada.")
        
        self.best_params = study.best_params
        self.best_value = study.best_value
        best_model_name = self.best_params.get('model_name')
        
        # If model_name was forced (not in params), retrieve from user_attrs
        if not best_model_name and hasattr(study, 'best_trial'):
            best_model_name = study.best_trial.user_attrs.get('model_name')
        
        # FIX: Ensure best_params includes model_name
        if best_model_name:
            self.best_params['model_name'] = best_model_name
        
        logger.info(f"üèÜ Melhor modelo global encontrado: {best_model_name}")
        logger.info(f"üìä Melhores par√¢metros: {self.best_params}")
        
        if self.task_type == 'time_series':
            self.best_params.update(self.ts_metadata)
        
        self.best_model = self._instantiate_model(best_model_name, self.best_params)
        
        # Reativar probabilidade para o modelo final se for SVM
        if best_model_name == 'svm' and hasattr(self.best_model, 'probability'):
            self.best_model.set_params(probability=True)
            logger.info("üîÆ Ativando estimativa de probabilidade para o modelo SVM final.")
        
        # Garantir a mesma seed no modelo final
        final_model_seed = self.random_state
        if isinstance(self.random_state, dict):
            final_model_seed = self.random_state.get(best_model_name, 42)
            
        if hasattr(self.best_model, 'random_state'):
            self.best_model.set_params(random_state=final_model_seed)
        elif hasattr(self.best_model, 'random_seed'):
            self.best_model.set_params(random_seed=final_model_seed)

        # Check for Transformers input
        final_X = X_train
        if isinstance(self.best_model, TransformersWrapper):
            if X_raw is not None:
                final_X = X_raw
                logger.info(f"ü§ñ Final Fit: Model {best_model_name} is a Transformer. Using RAW TEXT input.")
            else:
                 logger.warning(f"‚ö†Ô∏è Final Fit: Model {best_model_name} is a Transformer but X_raw is missing. Expect failure.")

        if y_train is not None:
            self.best_model.fit(final_X, y_train)
        else:
            self.best_model.fit(final_X)
            
        # Adicionar Feature Importance se dispon√≠vel
        if hasattr(self.best_model, 'feature_importances_'):
            self.feature_importance = self.best_model.feature_importances_.tolist()
            logger.info("üìà Feature Importance calculada.")
        elif hasattr(self.best_model, 'coef_'):
            self.feature_importance = np.abs(self.best_model.coef_).flatten().tolist()
            logger.info("üìà Feature Importance calculada (baseada em coeficientes).")
        else:
            self.feature_importance = None

        return self.best_model

    def get_supported_models(self):
        """Returns a list of supported model names for the current task type."""
        models = []
        if self.task_type == 'classification':
            models = [
                'logistic_regression', 'random_forest', 'xgboost', 'lightgbm', 
                'catboost', 'extra_trees', 'adaboost', 'decision_tree', 
                'svm', 'knn', 'naive_bayes', 'sgd_classifier', 'mlp', 'linear_svc', 'ridge_classifier',
                'voting_ensemble', 'custom_voting', 'custom_stacking'
            ]
            if TRANSFORMERS_AVAILABLE:
                models.extend([
                    'bert-base-uncased', 'distilbert-base-uncased', 'roberta-base', 
                    'albert-base-v2', 'xlnet-base-cased', 'microsoft/deberta-v3-base'
                ])
            return models
        elif self.task_type == 'regression':
            models = [
                'linear_regression', 'random_forest', 'xgboost', 'lightgbm', 
                'catboost', 'extra_trees', 'adaboost', 'decision_tree', 
                'svm', 'knn', 'ridge', 'lasso', 'elastic_net', 
                'sgd_regressor', 'mlp',
                'custom_voting', 'custom_stacking'
            ]
            if TRANSFORMERS_AVAILABLE:
                models.extend([
                    'bert-base-uncased', 'distilbert-base-uncased', 'roberta-base', 
                    'microsoft/deberta-v3-small'
                ])
            return models
        elif self.task_type == 'clustering':
            return [
                'kmeans', 'dbscan', 'agglomerative', 'gaussian_mixture', 
                'spectral', 'mean_shift', 'birch'
            ]
        elif self.task_type == 'anomaly_detection':
            return [
                'isolation_forest', 'one_class_svm', 'local_outlier_factor', 
                'elliptic_envelope'
            ]
        return []

    def create_model_instance(self, model_name, params=None):
        """Creates a model instance with given parameters (no prefixes expected)."""
        if params is None:
            params = {}
            
        # Clean params (remove None values or empty strings that might come from UI)
        clean_params = {k: v for k, v in params.items() if v is not None and v != ""}
        
        try:
            if self.task_type == 'classification':
                if model_name == 'logistic_regression': return LogisticRegression(max_iter=2000, **clean_params)
                if model_name == 'random_forest': return RandomForestClassifier(**clean_params)
                if model_name == 'xgboost': return xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', **clean_params)
                if model_name == 'lightgbm': return lgb.LGBMClassifier(verbosity=-1, **clean_params)
                if model_name == 'catboost' and CATBOOST_AVAILABLE: return cb.CatBoostClassifier(verbose=0, **clean_params)
                if model_name == 'extra_trees': return ExtraTreesClassifier(**clean_params)
                if model_name == 'adaboost': return AdaBoostClassifier(**clean_params)
                if model_name == 'decision_tree': return DecisionTreeClassifier(**clean_params)
                if model_name == 'svm': return SVC(probability=True, **clean_params)
                if model_name == 'knn': return KNeighborsClassifier(**clean_params)
                if model_name == 'naive_bayes': return GaussianNB(**clean_params)
                if model_name == 'sgd_classifier': return SGDClassifier(max_iter=1000, **clean_params)
                if model_name == 'mlp': return MLPClassifier(max_iter=1000, **clean_params)
                if model_name == 'linear_svc': return LinearSVC(**clean_params)
                if model_name == 'ridge_classifier': return RidgeClassifier(**clean_params)
                if model_name == 'voting_ensemble':
                    # Default God Mode Ensemble
                    return VotingClassifier(
                        estimators=[
                            ('pa', PassiveAggressiveClassifier(max_iter=1000, random_state=42, C=0.5)),
                            ('lr', LogisticRegression(max_iter=2000, C=10, solver='saga', n_jobs=-1, random_state=42)),
                            ('sgd', SGDClassifier(loss='modified_huber', max_iter=2000, n_jobs=-1, random_state=42))
                        ],
                        voting='hard',
                        n_jobs=-1
                    )
                
                if model_name == 'custom_voting':
                    ensemble_config = params.get('ensemble_config', {})
                    return VotingClassifier(
                        estimators=self._resolve_estimators(
                            ensemble_config.get('voting_estimators', []),
                            42
                        ),
                        voting=ensemble_config.get('voting_type', 'soft'),
                        weights=ensemble_config.get('voting_weights', None),
                        n_jobs=-1
                    )
                
                if model_name == 'custom_stacking':
                    ensemble_config = params.get('ensemble_config', {})
                    final_est_name = ensemble_config.get('stacking_final_estimator', 'logistic_regression')
                    return StackingClassifier(
                        estimators=self._resolve_estimators(
                            ensemble_config.get('stacking_estimators', []),
                            42
                        ),
                        final_estimator=self._get_default_model(final_est_name, 42) if isinstance(final_est_name, str) else final_est_name,
                        n_jobs=-1
                    )

            elif self.task_type == 'regression':
                if model_name == 'custom_voting':
                    ensemble_config = params.get('ensemble_config', {})
                    return VotingRegressor(
                        estimators=self._resolve_estimators(
                            ensemble_config.get('voting_estimators', []),
                            42
                        ),
                        weights=ensemble_config.get('voting_weights', None),
                        n_jobs=-1
                    )
                
                if model_name == 'custom_stacking':
                    ensemble_config = params.get('ensemble_config', {})
                    final_est_name = ensemble_config.get('stacking_final_estimator', 'linear_regression')
                    return StackingRegressor(
                        estimators=self._resolve_estimators(
                            ensemble_config.get('stacking_estimators', []),
                            42
                        ),
                        final_estimator=self._get_default_model(final_est_name, 42) if isinstance(final_est_name, str) else final_est_name,
                        n_jobs=-1
                    )
                
                if model_name == 'linear_regression': return LinearRegression(**clean_params)
                if model_name == 'random_forest': return RandomForestRegressor(**clean_params)
                if model_name == 'xgboost': return xgb.XGBRegressor(objective='reg:squarederror', **clean_params)
                if model_name == 'lightgbm': return lgb.LGBMRegressor(verbosity=-1, **clean_params)
                if model_name == 'catboost' and CATBOOST_AVAILABLE: return cb.CatBoostRegressor(verbose=0, **clean_params)
                if model_name == 'extra_trees': return ExtraTreesRegressor(**clean_params)
                if model_name == 'adaboost': return AdaBoostRegressor(**clean_params)
                if model_name == 'decision_tree': return DecisionTreeRegressor(**clean_params)
                if model_name == 'svm': return SVR(**clean_params)
                if model_name == 'knn': return KNeighborsRegressor(**clean_params)
                if model_name == 'ridge': return Ridge(**clean_params)
                if model_name == 'lasso': return Lasso(**clean_params)
                if model_name == 'elastic_net': return ElasticNet(**clean_params)
                if model_name == 'sgd_regressor': return SGDRegressor(max_iter=1000, **clean_params)
                if model_name == 'mlp': return MLPRegressor(max_iter=1000, **clean_params)

            elif self.task_type == 'clustering':
                if model_name == 'kmeans': return KMeans(n_init=10, **clean_params)
                if model_name == 'dbscan': return DBSCAN(**clean_params)
                if model_name == 'agglomerative': return AgglomerativeClustering(**clean_params)
                if model_name == 'gaussian_mixture': return GaussianMixture(**clean_params)
                if model_name == 'spectral': return SpectralClustering(**clean_params)
                if model_name == 'mean_shift': return MeanShift(**clean_params)
                if model_name == 'birch': return Birch(**clean_params)

            elif self.task_type == 'anomaly_detection':
                if model_name == 'isolation_forest': return IsolationForest(**clean_params)
                if model_name == 'one_class_svm': return OneClassSVM(**clean_params)
                if model_name == 'local_outlier_factor': return LocalOutlierFactor(novelty=True, **clean_params)
                if model_name == 'elliptic_envelope': return EllipticEnvelope(**clean_params)
                
        except Exception as e:
            logger.error(f"Error creating model instance for {model_name}: {e}")
            return None
            
        return None

    def _instantiate_model(self, name, params):
        # 1. Custom Models (Uploaded/Registered)
        if hasattr(self, 'custom_models') and self.custom_models and name in self.custom_models:
            logger.info(f"Using custom model: {name}")
            return self.custom_models[name]

        # 2. Transformers
        if TRANSFORMERS_AVAILABLE and (name.startswith('bert') or name.startswith('roberta') or name.startswith('distilbert') or name.startswith('albert') or name.startswith('xlnet') or '/' in name):
             epochs = params.get('num_train_epochs', 3)
             lr = params.get('learning_rate', 2e-5)
             return TransformersWrapper(model_name=name, task=self.task_type, epochs=epochs, learning_rate=lr)

        if self.task_type == 'classification':
            if name == 'custom_voting':
                return VotingClassifier(
                    estimators=self._resolve_estimators(
                        self.ensemble_config.get('voting_estimators', [
                            ('lr', LogisticRegression(random_state=42)), 
                            ('rf', RandomForestClassifier(random_state=42))
                        ]),
                        42 # Default random_state for instantiation
                    ),
                    voting=self.ensemble_config.get('voting_type', 'soft'),
                    weights=self.ensemble_config.get('voting_weights', None),
                    n_jobs=-1
                )
            if name == 'custom_stacking':
                return StackingClassifier(
                    estimators=self._resolve_estimators(
                        self.ensemble_config.get('stacking_estimators', [
                            ('rf', RandomForestClassifier(random_state=42)),
                            ('svm', SVC(probability=True, random_state=42))
                        ]),
                        42
                    ),
                    final_estimator=self.ensemble_config.get('stacking_final_estimator', LogisticRegression(random_state=42)),
                    n_jobs=-1
                )
            if name == 'logistic_regression':  
                lr_params = {k.replace('lr_', ''): v for k, v in params.items() if k.startswith('lr_')}
                return LogisticRegression(max_iter=2000, **lr_params)
            if name == 'random_forest': 
                rf_params = {k.replace('rf_', ''): v for k, v in params.items() if k.startswith('rf_')}
                return RandomForestClassifier(**rf_params)
            if name == 'xgboost': 
                xgb_params = {k.replace('xgb_', ''): v for k, v in params.items() if k.startswith('xgb_')}
                return xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', **xgb_params)
            if name == 'lightgbm': 
                lgb_params = {k.replace('lgb_', ''): v for k, v in params.items() if k.startswith('lgb_')}
                return lgb.LGBMClassifier(verbosity=-1, **lgb_params)
            if name == 'extra_trees':
                et_params = {k.replace('et_', ''): v for k, v in params.items() if k.startswith('et_')}
                return ExtraTreesClassifier(**et_params)
            if name == 'adaboost':
                ada_params = {k.replace('ada_', ''): v for k, v in params.items() if k.startswith('ada_')}
                return AdaBoostClassifier(**ada_params)
            if name == 'decision_tree':
                dt_params = {k.replace('dt_', ''): v for k, v in params.items() if k.startswith('dt_')}
                return DecisionTreeClassifier(**dt_params)
            if name == 'svm': return SVC(probability=True, **{k.replace('svm_', ''): v for k, v in params.items() if k.startswith('svm_') or k in ['C', 'kernel', 'gamma', 'degree', 'coef0', 'class_weight', 'shrinking', 'tol']})
            if name == 'linear_svc': 
                lsvc_params = {k: v for k, v in params.items() if k in ['C', 'loss', 'penalty', 'dual', 'class_weight', 'fit_intercept']}
                # Default dual=False if not specified to avoid convergence issues with primal
                if 'dual' not in lsvc_params and lsvc_params.get('penalty') == 'l2' and lsvc_params.get('loss') == 'hinge':
                     lsvc_params['dual'] = True # hinge requires dual=True
                elif 'dual' not in lsvc_params:
                     lsvc_params['dual'] = False
                return LinearSVC(max_iter=2000, **lsvc_params)
            if name == 'knn': 
                knn_params = {k.replace('knn_', ''): v for k, v in params.items() if k.startswith('knn_')}
                if 'neighbors' in knn_params:
                    knn_params['n_neighbors'] = knn_params.pop('neighbors')
                return KNeighborsClassifier(**knn_params)
            if name == 'naive_bayes': 
                nb_params = {k.replace('nb_', ''): v for k, v in params.items() if k.startswith('nb_')}
                return GaussianNB(**nb_params)
            if name == 'ridge_classifier': 
                rc_params = {k.replace('rc_', ''): v for k, v in params.items() if k.startswith('rc_')}
                if 'alpha' not in rc_params and 'ridge_alpha' in params: rc_params['alpha'] = params['ridge_alpha']
                return RidgeClassifier(**rc_params)
            if name == 'sgd_classifier': 
                sgd_params = {k.replace('sgd_', ''): v for k, v in params.items() if k.startswith('sgd_')}
                return SGDClassifier(max_iter=2000, **sgd_params)
            if name == 'mlp': 
                mlp_params = {k.replace('mlp_', ''): v for k, v in params.items() if k.startswith('mlp_')}
                if 'layers' in mlp_params:
                    mlp_params['hidden_layer_sizes'] = mlp_params.pop('layers')
                return MLPClassifier(early_stopping=True, **mlp_params)
            if name == 'catboost':
                cb_params = {k.replace('cb_', ''): v for k, v in params.items() if k.startswith('cb_')}
                return cb.CatBoostClassifier(verbose=0, thread_count=-1, **cb_params)
            if name == 'voting_ensemble':
                # Default God Mode Ensemble
                return VotingClassifier(
                    estimators=[
                        ('pa', PassiveAggressiveClassifier(max_iter=1000, random_state=42, C=0.5)),
                        ('lr', LogisticRegression(max_iter=2000, C=10, solver='saga', n_jobs=-1, random_state=42)),
                        ('sgd', SGDClassifier(loss='modified_huber', max_iter=2000, n_jobs=-1, random_state=42))
                    ],
                    voting='hard',
                    n_jobs=-1
                )
        elif self.task_type == 'regression':
            if name == 'custom_voting':
                return VotingRegressor(
                    estimators=self._resolve_estimators(
                        self.ensemble_config.get('voting_estimators', [
                            ('lr', LinearRegression()), 
                            ('rf', RandomForestRegressor(random_state=42))
                        ]),
                        42
                    ),
                    weights=self.ensemble_config.get('voting_weights', None),
                    n_jobs=-1
                )
            if name == 'custom_stacking':
                return StackingRegressor(
                    estimators=self._resolve_estimators(
                        self.ensemble_config.get('stacking_estimators', [
                            ('rf', RandomForestRegressor(random_state=42)),
                            ('svm', SVR())
                        ]),
                        42
                    ),
                    final_estimator=self.ensemble_config.get('stacking_final_estimator', LinearRegression()),
                    n_jobs=-1
                )
            if name == 'linear_regression': return LinearRegression()
            if name == 'random_forest': 
                rf_params = {k.replace('rf_', ''): v for k, v in params.items() if k.startswith('rf_')}
                return RandomForestRegressor(**rf_params)
            if name == 'xgboost': 
                xgb_params = {k.replace('xgb_', ''): v for k, v in params.items() if k.startswith('xgb_')}
                return xgb.XGBRegressor(**xgb_params)
            if name == 'lightgbm': 
                lgb_params = {k.replace('lgb_', ''): v for k, v in params.items() if k.startswith('lgb_')}
                return lgb.LGBMRegressor(verbosity=-1, **lgb_params)
            if name == 'extra_trees':
                et_params = {k.replace('et_', ''): v for k, v in params.items() if k.startswith('et_')}
                return ExtraTreesRegressor(**et_params)
            if name == 'adaboost':
                ada_params = {k.replace('ada_', ''): v for k, v in params.items() if k.startswith('ada_')}
                return AdaBoostRegressor(**ada_params)
            if name == 'decision_tree':
                dt_params = {k.replace('dt_', ''): v for k, v in params.items() if k.startswith('dt_')}
                return DecisionTreeRegressor(**dt_params)
            if name == 'svm': return SVR(**{k.replace('svm_', ''): v for k, v in params.items() if k.startswith('svm_') or k in ['C', 'kernel', 'gamma', 'epsilon', 'degree', 'coef0']})
            if name == 'knn': 
                knn_params = {k.replace('knn_', ''): v for k, v in params.items() if k.startswith('knn_')}
                if 'neighbors' in knn_params:
                    knn_params['n_neighbors'] = knn_params.pop('neighbors')
                return KNeighborsRegressor(**knn_params)
            if name == 'ridge': return Ridge(alpha=params.get('ridge_alpha', 1.0))
            if name == 'lasso': return Lasso(alpha=params.get('lasso_alpha', 1.0))
            if name == 'elastic_net': 
                return ElasticNet(alpha=params.get('en_alpha', 1.0), l1_ratio=params.get('en_l1_ratio', 0.5))
            if name == 'sgd_regressor': 
                sgd_params = {k.replace('sgd_', ''): v for k, v in params.items() if k.startswith('sgd_')}
                return SGDRegressor(max_iter=1000, **sgd_params)
            if name == 'mlp': 
                mlp_params = {k.replace('mlp_', ''): v for k, v in params.items() if k.startswith('mlp_')}
                if 'layers' in mlp_params:
                    mlp_params['hidden_layer_sizes'] = mlp_params.pop('layers')
                return MLPRegressor(early_stopping=True, **mlp_params)
            if name == 'catboost':
                cb_params = {k.replace('cb_', ''): v for k, v in params.items() if k.startswith('cb_')}
                return cb.CatBoostRegressor(verbose=0, thread_count=-1, **cb_params)
        elif self.task_type == 'clustering':
            if name == 'kmeans': 
                return KMeans(n_clusters=params.get('km_n_clusters', 8), n_init=10)
            if name == 'agglomerative':
                return AgglomerativeClustering(n_clusters=params.get('agg_n_clusters', 2))
            if name == 'dbscan':
                return DBSCAN(eps=params.get('db_eps', 0.5), min_samples=params.get('db_min_samples', 5))
            if name == 'gaussian_mixture':
                return GaussianMixture(n_components=params.get('gm_n_components', 1))
            if name == 'mean_shift': return MeanShift()
            if name == 'birch': 
                return Birch(n_clusters=params.get('birch_n_clusters', 3))
            if name == 'spectral':
                return SpectralClustering(n_clusters=params.get('spectral_n_clusters', 3))
        elif self.task_type == 'time_series':
            if name == 'random_forest_ts':
                return RandomForestRegressor(n_estimators=params.get('rf_ts_n_estimators', 100))
            if name == 'xgboost_ts':
                return xgb.XGBRegressor(n_estimators=params.get('xgb_ts_n_estimators', 100))
            if name == 'extra_trees_ts':
                return ExtraTreesRegressor(n_estimators=params.get('et_ts_n_estimators', 100))
            if name == 'catboost_ts':
                cb_params = {k.replace('cb_ts_', ''): v for k, v in params.items() if k.startswith('cb_ts_')}
                return cb.CatBoostRegressor(verbose=0, thread_count=-1, **cb_params)
        elif self.task_type == 'anomaly_detection':
            if name == 'isolation_forest':
                if_params = {k.replace('if_', ''): v for k, v in params.items() if k.startswith('if_')}
                return IsolationForest(random_state=42, **if_params)
            if name == 'local_outlier_factor':
                lof_params = {k.replace('lof_', ''): v for k, v in params.items() if k.startswith('lof_')}
                if 'neighbors' in lof_params: lof_params['n_neighbors'] = lof_params.pop('neighbors')
                return LocalOutlierFactor(novelty=True, **lof_params)
            if name == 'elliptic_envelope':
                ee_params = {k.replace('ee_', ''): v for k, v in params.items() if k.startswith('ee_')}
                return EllipticEnvelope(random_state=42, **ee_params)
            if name == 'one_class_svm':
                oc_params = {k.replace('oc_', ''): v for k, v in params.items() if k.startswith('oc_')}
                return OneClassSVM(**oc_params)

    def evaluate(self, X_test, y_test=None):
        if y_test is not None:
            y_pred = self.best_model.predict(X_test)
            metrics = {}
            
            if self.task_type == 'classification':
                metrics['accuracy'] = accuracy_score(y_test, y_pred)
                metrics['balanced_accuracy'] = balanced_accuracy_score(y_test, y_pred)
                metrics['precision'] = precision_score(y_test, y_pred, average='weighted')
                metrics['recall'] = recall_score(y_test, y_pred, average='weighted')
                metrics['f1'] = f1_score(y_test, y_pred, average='weighted')
                metrics['kappa'] = cohen_kappa_score(y_test, y_pred)
                metrics['mcc'] = matthews_corrcoef(y_test, y_pred)
                metrics['confusion_matrix'] = confusion_matrix(y_test, y_pred).tolist()
                try:
                    y_prob = self.best_model.predict_proba(X_test)
                    metrics['roc_auc'] = roc_auc_score(y_test, y_prob, multi_class='ovr')
                    metrics['log_loss'] = log_loss(y_test, y_prob)
                except:
                    metrics['roc_auc'] = 0.5
                    metrics['log_loss'] = 0.0
            elif self.task_type in ['regression', 'time_series']:
                metrics['rmse'] = np.sqrt(mean_squared_error(y_test, y_pred))
                metrics['mae'] = mean_absolute_error(y_test, y_pred)
                metrics['median_ae'] = median_absolute_error(y_test, y_pred)
                metrics['r2'] = r2_score(y_test, y_pred)
                metrics['explained_variance'] = explained_variance_score(y_test, y_pred)
                metrics['mape'] = mean_absolute_percentage_error(y_test, y_pred)
                try:
                    metrics['msle'] = mean_squared_log_error(y_test, np.clip(y_pred, 0, None))
                except:
                    metrics['msle'] = 0.0
            
            elif self.task_type == 'anomaly_detection':
                # For anomaly detection, -1 is anomaly, 1 is normal
                # If y_test is provided, we assume it has labels (0 for normal, 1 for anomaly)
                # and map predictions accordingly
                y_pred_mapped = np.where(y_pred == -1, 1, 0)
                metrics['accuracy'] = accuracy_score(y_test, y_pred_mapped)
                metrics['f1'] = f1_score(y_test, y_pred_mapped)
                metrics['n_anomalies'] = int(np.sum(y_pred == -1))
                
            return metrics, y_pred
        else:
            # Clustering or Anomaly Detection without y_test
            if self.task_type == 'clustering':
                labels = self.best_model.labels_ if hasattr(self.best_model, 'labels_') else self.best_model.predict(X_test)
                metrics = {
                    'silhouette': silhouette_score(X_test, labels) if len(set(labels)) > 1 else -1,
                    'calinski_harabasz': calinski_harabasz_score(X_test, labels) if len(set(labels)) > 1 else -1,
                    'davies_bouldin': davies_bouldin_score(X_test, labels) if len(set(labels)) > 1 else -1,
                    'n_clusters': len(set(labels))
                }
                return metrics, labels
            elif self.task_type == 'anomaly_detection':
                y_pred = self.best_model.predict(X_test)
                metrics = {
                    'n_anomalies': int(np.sum(y_pred == -1)),
                    'anomaly_ratio': float(np.sum(y_pred == -1) / len(y_pred))
                }
                return metrics, y_pred

    def get_model_params_schema(self, model_name):
        """Returns the parameter schema for manual fine-tuning, matching _get_models keys."""
        schemas = {
            'logistic_regression': {
                'lr_C': ('float', 0.001, 100.0, 1.0),
                'lr_solver': ('list', ['lbfgs', 'liblinear', 'saga', 'newton-cg'], 'lbfgs'),
                'lr_penalty': ('list', ['l2', 'l1', 'elasticnet', None], 'l2'),
                'lr_l1_ratio': ('float', 0.0, 1.0, 0.5),
                'lr_class_weight': ('list', [None, 'balanced'], None),
                'lr_fit_intercept': ('list', [True, False], True),
                'lr_max_iter': ('int', 100, 5000, 2000),
                'lr_tol': ('float', 1e-5, 1e-3, 1e-4)
            },
            'linear_svc': {
                'C': ('float', 0.001, 100.0, 1.0),
                'loss': ('list', ['hinge', 'squared_hinge'], 'squared_hinge'),
                'penalty': ('list', ['l1', 'l2'], 'l2'),
                'class_weight': ('list', [None, 'balanced'], None),
                'fit_intercept': ('list', [True, False], True),
                'dual': ('list', [True, False, 'auto'], 'auto'),
                'max_iter': ('int', 1000, 10000, 2000),
                'tol': ('float', 1e-5, 1e-3, 1e-4)
            },
            'ridge_classifier': {
                'rc_alpha': ('float', 0.01, 100.0, 1.0),
                'rc_solver': ('list', ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga'], 'auto'),
                'rc_class_weight': ('list', [None, 'balanced'], None),
                'rc_fit_intercept': ('list', [True, False], True)
            },
            'sgd_classifier': {
                'sgd_alpha': ('float', 1e-6, 1e-1, 0.0001),
                'sgd_penalty': ('list', ['l2', 'l1', 'elasticnet'], 'l2'),
                'sgd_loss': ('list', ['hinge', 'log_loss', 'modified_huber', 'squared_hinge', 'perceptron'], 'hinge'),
                'sgd_learning_rate': ('list', ['optimal', 'constant', 'invscaling', 'adaptive'], 'optimal'),
                'sgd_eta0': ('float', 0.0, 1.0, 0.0),
                'sgd_class_weight': ('list', [None, 'balanced'], None),
                'sgd_max_iter': ('int', 100, 5000, 1000),
                'sgd_tol': ('float', 1e-5, 1e-3, 1e-3)
            },
            'sgd_regressor': {
                'sgd_alpha': ('float', 1e-6, 1e-1, 0.0001),
                'sgd_penalty': ('list', ['l2', 'l1', 'elasticnet'], 'l2'),
                'sgd_loss': ('list', ['squared_error', 'huber', 'epsilon_insensitive', 'squared_epsilon_insensitive'], 'squared_error'),
                'sgd_learning_rate': ('list', ['invscaling', 'constant', 'optimal', 'adaptive'], 'invscaling'),
                'sgd_eta0': ('float', 0.0, 1.0, 0.01),
                'sgd_max_iter': ('int', 100, 5000, 1000),
                'sgd_tol': ('float', 1e-5, 1e-3, 1e-3)
            },
            'random_forest': {
                'rf_n_estimators': ('int', 10, 500, 100),
                'rf_max_depth': ('int', 1, 50, 10),
                'rf_min_samples_split': ('int', 2, 20, 2),
                'rf_min_samples_leaf': ('int', 1, 20, 1),
                'rf_max_features': ('list', ['sqrt', 'log2', None], 'sqrt'),
                'rf_bootstrap': ('list', [True, False], True),
                'rf_class_weight': ('list', [None, 'balanced', 'balanced_subsample'], None),
                'rf_criterion': ('list', ['gini', 'entropy', 'log_loss'], 'gini')
            },
            'xgboost': {
                'xgb_n_estimators': ('int', 50, 1000, 100),
                'xgb_lr': ('float', 0.01, 0.3, 0.1),
                'xgb_subsample': ('float', 0.5, 1.0, 1.0),
                'xgb_colsample_bytree': ('float', 0.5, 1.0, 1.0),
                'xgb_gamma': ('float', 0.0, 5.0, 0.0),
                'xgb_min_child_weight': ('float', 1.0, 10.0, 1.0),
                'xgb_scale_pos_weight': ('float', 1.0, 10.0, 1.0),
                'xgb_reg_alpha': ('float', 0.0, 10.0, 0.0),
                'xgb_reg_lambda': ('float', 0.0, 10.0, 1.0),
                'xgb_tree_method': ('list', ['auto', 'exact', 'approx', 'hist'], 'auto'),
                'xgb_max_leaves': ('int', 0, 100, 0),
                'xgb_grow_policy': ('list', ['depthwise', 'lossguide'], 'depthwise')
            },
            'lightgbm': {
                'lgb_n_estimators': ('int', 50, 1000, 100),
                'lgb_lr': ('float', 0.01, 0.3, 0.1),
                'lgb_num_leaves': ('int', 20, 150, 31),
                'lgb_min_child_samples': ('int', 10, 100, 20),
                'lgb_subsample': ('float', 0.5, 1.0, 1.0),
                'lgb_colsample_bytree': ('float', 0.5, 1.0, 1.0),
                'lgb_scale_pos_weight': ('float', 1.0, 10.0, 1.0),
                'lgb_reg_alpha': ('float', 0.0, 10.0, 0.0),
                'lgb_reg_lambda': ('float', 0.0, 10.0, 0.0),
                'lgb_boosting_type': ('list', ['gbdt', 'dart', 'goss'], 'gbdt'),
                'lgb_min_split_gain': ('float', 0.0, 1.0, 0.0),
                'lgb_class_weight': ('list', [None, 'balanced'], None)
            },
            'extra_trees': {
                'et_n_estimators': ('int', 10, 500, 100),
                'et_max_depth': ('int', 1, 50, 10),
                'et_min_samples_split': ('int', 2, 20, 2),
                'et_min_samples_leaf': ('int', 1, 20, 1),
                'et_max_features': ('list', ['sqrt', 'log2', None], 'sqrt'),
                'et_bootstrap': ('list', [True, False], False),
                'et_class_weight': ('list', [None, 'balanced', 'balanced_subsample'], None),
                'et_criterion': ('list', ['gini', 'entropy', 'log_loss'], 'gini')
            },
            'adaboost': {
                'ada_n_estimators': ('int', 10, 500, 50),
                'ada_lr': ('float', 0.01, 1.0, 1.0)
            },
            'decision_tree': {
                'dt_max_depth': ('int', 1, 50, 10),
                'dt_min_samples_split': ('int', 2, 20, 2),
                'dt_min_samples_leaf': ('int', 1, 20, 1),
                'dt_criterion': ('list', ['gini', 'entropy', 'log_loss'], 'gini') if self.task_type == 'classification' else ('list', ['squared_error', 'friedman_mse', 'absolute_error', 'poisson'], 'squared_error'),
                'dt_splitter': ('list', ['best', 'random'], 'best'),
                'dt_class_weight': ('list', [None, 'balanced'], None)
            },
            'mlp': {
                'mlp_layers': ('list', ["(50,)", "(100,)", "(50, 50)", "(100, 50)"], "(100,)"),
                'mlp_max_iter': ('int', 100, 2000, 500),
                'mlp_activation': ('list', ['identity', 'logistic', 'tanh', 'relu'], 'relu'),
                'mlp_solver': ('list', ['lbfgs', 'sgd', 'adam'], 'adam'),
                'mlp_alpha': ('float', 0.0001, 0.1, 0.0001),
                'mlp_learning_rate': ('list', ['constant', 'invscaling', 'adaptive'], 'constant'),
                'mlp_batch_size': ('list', ['auto', 32, 64, 128], 'auto'),
                'mlp_tol': ('float', 1e-5, 1e-3, 1e-4)
            },
            'knn': {
                'knn_neighbors': ('int', 1, 30, 5),
                'knn_weights': ('list', ['uniform', 'distance'], 'uniform'),
                'knn_metric': ('list', ['minkowski', 'euclidean', 'manhattan'], 'minkowski'),
                'knn_p': ('int', 1, 5, 2),
                'knn_algorithm': ('list', ['auto', 'ball_tree', 'kd_tree', 'brute'], 'auto')
            },
            'naive_bayes': {
                'nb_var_smoothing': ('float', 1e-10, 1e-7, 1e-9)
            },
            'svm': {
                'C': ('float', 0.1, 100.0, 1.0),
                'kernel': ('list', ['linear', 'poly', 'rbf', 'sigmoid'], 'rbf'),
                'gamma': ('list', ['scale', 'auto'], 'scale'),
                'degree': ('int', 2, 5, 3),
                'coef0': ('float', 0.0, 10.0, 0.0),
                'class_weight': ('list', [None, 'balanced'], None),
                'shrinking': ('list', [True, False], True),
                'tol': ('float', 1e-4, 1e-2, 1e-3)
            },
            'ridge': {
                'ridge_alpha': ('float', 0.1, 10.0, 1.0)
            },
            'lasso': {
                'lasso_alpha': ('float', 0.01, 1.0, 0.1)
            },
            'elastic_net': {
                'en_alpha': ('float', 0.01, 1.0, 0.1),
                'en_l1_ratio': ('float', 0.0, 1.0, 0.5)
            },
            'catboost': {
                'cb_iterations': ('int', 10, 1000, 100),
                'cb_lr': ('float', 0.001, 0.3, 0.1),
                'cb_depth': ('int', 1, 12, 6),
                'cb_l2_leaf_reg': ('float', 1.0, 10.0, 3.0),
                'cb_border_count': ('int', 32, 255, 254)
            },
            'catboost_ts': {
                'cb_ts_iterations': ('int', 10, 1000, 100)
            },
            'random_forest_ts': {
                'rf_ts_n_estimators': ('int', 50, 200, 100)
            },
            'xgboost_ts': {
                'xgb_ts_n_estimators': ('int', 50, 200, 100)
            },
            'extra_trees_ts': {
                'et_ts_n_estimators': ('int', 50, 200, 100)
            },
            'isolation_forest': {
                'if_n_estimators': ('int', 50, 200, 100),
                'if_contamination': ('float', 0.01, 0.2, 0.1)
            },
            'local_outlier_factor': {
                'lof_neighbors': ('int', 10, 50, 20),
                'lof_contamination': ('float', 0.01, 0.2, 0.1)
            },
            'elliptic_envelope': {
                'ee_contamination': ('float', 0.01, 0.2, 0.1)
            },
            'one_class_svm': {
                'oc_nu': ('float', 0.01, 0.2, 0.1),
                'oc_kernel': ('list', ['rbf', 'poly', 'sigmoid'], 'rbf')
            },
            'kmeans': {
                'km_n_clusters': ('int', 2, 20, 8)
            },
            'agglomerative': {
                'agg_n_clusters': ('int', 2, 20, 2)
            },
            'dbscan': {
                'db_eps': ('float', 0.1, 5.0, 0.5),
                'db_min_samples': ('int', 1, 20, 5)
            },
            'gaussian_mixture': {
                'gm_n_components': ('int', 1, 20, 1)
            },
            'birch': {
                'birch_n_clusters': ('int', 2, 20, 3)
            },
            'spectral': {
                'spectral_n_clusters': ('int', 2, 20, 3)
            }
        }

        if TRANSFORMERS_AVAILABLE:
             # Add schema for all transformer models (same parameters for all)
             transformer_models = [
                 'bert-base-uncased', 'distilbert-base-uncased', 'roberta-base', 
                 'albert-base-v2', 'xlnet-base-cased', 'microsoft/deberta-v3-base',
                 'microsoft/deberta-v3-small'
             ]
             for tm in transformer_models:
                 schemas[tm] = {
                     'learning_rate': ('float', 1e-6, 1e-3, 2e-5),
                     'num_train_epochs': ('int', 1, 20, 3)
                 }

        return schemas.get(model_name, {})

    def train_manual(self, X_train, y_train, model_name, params, **kwargs):
        """Trains a specific model with user-provided hyperparameters."""
        # Time Series specific feature engineering (Simple Lags)
        if self.task_type == 'time_series' and y_train is not None:
            horizon = kwargs.get('forecast_horizon', 1)
            if isinstance(X_train, pd.DataFrame):
                X_train = X_train.copy()
                for i in range(1, horizon + 1):
                    X_train[f'lag_{i}'] = y_train.shift(i)
                X_train = X_train.fillna(method='bfill')

        self.best_model = self._instantiate_model(model_name, params)
        if y_train is not None:
            self.best_model.fit(X_train, y_train)
        else:
            self.best_model.fit(X_train)
        self.best_params = params
        return self.best_model

def get_technical_explanation(model_name, params, task_type):
    explanations = {
        'classification': {
            'random_forest': "Random Forest √© um conjunto de √°rvores de decis√£o. √â robusto a outliers e reduz o overfitting combinando m√∫ltiplas √°rvores.",
            'xgboost': "XGBoost usa Gradient Boosting extremo. √â eficiente e poderoso, corrigindo erros de √°rvores anteriores sequencialmente.",
            'lightgbm': "LightGBM √© um framework de gradient boosting que usa algoritmos baseados em √°rvores. √â projetado para ser distribu√≠do e eficiente.",
            'extra_trees': "Extra Trees √© semelhante ao Random Forest, mas usa divis√µes aleat√≥rias mais extremas, o que pode reduzir ainda mais a vari√¢ncia.",
            'adaboost': "AdaBoost foca em inst√¢ncias que foram classificadas incorretamente por modelos anteriores, construindo um classificador forte a partir de fracos.",
            'decision_tree': "√Årvore de Decis√£o √© um modelo simples e interpret√°vel que divide os dados em ramos baseados em caracter√≠sticas.",
            'svm': "SVM encontra o hiperplano que melhor separa as classes no espa√ßo de caracter√≠sticas.",
            'mlp': "MLP √© uma rede neural artificial b√°sica capaz de aprender rela√ß√µes n√£o lineares complexas.",
            'logistic_regression': "Regress√£o Log√≠stica √© um modelo estat√≠stico usado para predi√ß√£o de classes bin√°rias ou multiclasse.",
            'linear_svc': "LinearSVC √© uma implementa√ß√£o r√°pida de SVM com kernel linear, eficiente para grandes datasets.",
            'knn': "K-Nearest Neighbors (KNN) classifica inst√¢ncias com base na classe majorit√°ria de seus vizinhos mais pr√≥ximos.",
            'naive_bayes': "Naive Bayes √© um classificador probabil√≠stico baseado no Teorema de Bayes com suposi√ß√µes de independ√™ncia forte.",
            'ridge_classifier': "Ridge Classifier usa regress√£o ridge para classifica√ß√£o, aplicando regulariza√ß√£o L2.",
            'sgd_classifier': "SGD Classifier usa Gradiente Descendente Estoc√°stico, sendo ideal para aprendizado online e grandes volumes de dados."
        },
        'regression': {
            'random_forest': "Random Forest para regress√£o combina m√∫ltiplas √°rvores para prever valores cont√≠nuos com alta estabilidade.",
            'xgboost': "XGBoost para regress√£o oferece alta performance e regulariza√ß√£o para evitar overfitting em dados tabulares.",
            'lightgbm': "LightGBM √© extremamente r√°pido para regress√£o em grandes datasets.",
            'extra_trees': "Extra Trees Regressor oferece um n√≠vel extra de aleatoriedade, sendo √∫til quando os dados t√™m muito ru√≠do.",
            'adaboost': "AdaBoost Regressor ajusta pesos de predi√ß√µes anteriores para melhorar a precis√£o em valores cont√≠nuos.",
            'decision_tree': "√Årvore de Decis√£o para regress√£o prev√™ valores baseando-se na m√©dia dos dados em cada folha.",
            'svm': "SVR tenta encontrar uma fun√ß√£o que se desvie de y no m√°ximo uma pequena quantidade para todos os dados de treino.",
            'mlp': "MLP para regress√£o pode capturar padr√µes num√©ricos altamente complexos e n√£o lineares.",
            'linear_regression': "Regress√£o Linear √© o modelo base que assume uma rela√ß√£o linear entre as vari√°veis.",
            'knn': "KNN Regressor estima o valor alvo baseando-se na m√©dia dos valores de seus vizinhos mais pr√≥ximos.",
            'ridge': "Ridge Regression aplica regulariza√ß√£o L2 para evitar que os coeficientes se tornem muito grandes.",
            'lasso': "Lasso Regression aplica regulariza√ß√£o L1, o que pode levar a coeficientes zero (sele√ß√£o de caracter√≠sticas).",
            'elastic_net': "Elastic Net combina regulariza√ß√£o L1 e L2, sendo √∫til quando h√° m√∫ltiplas caracter√≠sticas correlacionadas.",
            'sgd_regressor': "SGD Regressor aplica Gradiente Descendente Estoc√°stico para problemas de regress√£o em larga escala."
        },
        'clustering': {
            'kmeans': "K-Means agrupa dados minimizando a vari√¢ncia dentro de cada cluster (dist√¢ncia aos centroides).",
            'agglomerative': "Clustering Hier√°rquico Aglomerativo constr√≥i uma hierarquia de clusters de baixo para cima.",
            'dbscan': "DBSCAN identifica clusters baseados na densidade, sendo excelente para encontrar formas arbitr√°rias e ru√≠do.",
            'gaussian_mixture': "Gaussian Mixture assume que los dados s√£o gerados por uma mistura de v√°rias distribui√ß√µes gaussianas.",
            'mean_shift': "Mean Shift √© um algoritmo baseado em densidade que n√£o exige o n√∫mero de clusters a priori.",
            'birch': "BIRCH √© eficiente para clustering em grandes datasets, construindo uma √°rvore de caracter√≠sticas de cluster.",
            'spectral': "Spectral Clustering usa a conectividade dos dados para agrupar inst√¢ncias, √∫til para estruturas n√£o convexas."
        },
        'time_series': {
            'random_forest_ts': "Random Forest adaptado para s√©ries temporais usando lags e caracter√≠sticas temporais como preditores.",
            'xgboost_ts': "XGBoost para s√©ries temporais foca em capturar tend√™ncias e sazonalidades atrav√©s de boosting.",
            'extra_trees_ts': "Extra Trees para s√©ries temporais ajuda a mitigar o ru√≠do inerente a dados temporais."
        },
        'anomaly_detection': {
            'isolation_forest': "Isolation Forest isola anomalias selecionando aleatoriamente uma caracter√≠stica e um valor de divis√£o.",
            'local_outlier_factor': "LOF compara a densidade local de um ponto com a de seus vizinhos para identificar outliers.",
            'elliptic_envelope': "Assume que os dados normais v√™m de uma distribui√ß√£o gaussiana e detecta o que est√° fora do elipsoide.",
            'one_class_svm': "Aprende uma fronteira que envolve a maioria dos dados normais, classificando o que est√° fora como anomalia."
        }
    }
    
    model_desc = explanations.get(task_type, {}).get(model_name, "Modelo selecionado por sua performance superior durante a otimiza√ß√£o.")
    
    param_desc = []
    for k, v in params.items():
        if 'n_estimators' in k: param_desc.append(f"- **Estimadores ({v})**: N√∫mero de √°rvores no conjunto. Mais √°rvores aumentam a robustez, mas o custo computacional sobe.")
        if 'max_depth' in k: param_desc.append(f"- **Profundidade M√°xima ({v})**: Limita o crescimento da √°rvore para evitar overfitting.")
        if 'learning_rate' in k or 'lr' in k: param_desc.append(f"- **Taxa de Aprendizado ({v})**: Controla a magnitude do ajuste dos pesos a cada itera√ß√£o.")
        if 'n_clusters' in k: param_desc.append(f"- **N√∫mero de Clusters ({v})**: Quantidade de grupos que o algoritmo tentar√° identificar nos dados.")
        if 'contamination' in k: param_desc.append(f"- **Contamina√ß√£o ({v})**: A propor√ß√£o esperada de anomalias no conjunto de dados.")
        if 'n_neighbors' in k or 'neighbors' in k: param_desc.append(f"- **Vizinhos ({v})**: N√∫mero de vizinhos a serem usados para calcular a densidade local.")
        
    return model_desc, "\n".join(param_desc)

def save_pipeline(processor, model, path):
    os.makedirs(os.path.dirname(path), exist_ok=True)
    joblib.dump({'processor': processor, 'model': model}, path)

def load_pipeline(path):
    data = joblib.load(path)
    return data['processor'], data['model']
