"""
Script para analisar e comparar datasets de treino e valida√ß√£o
para entender varia√ß√µes de performance entre modelos
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
import warnings
warnings.filterwarnings('ignore')

def analyze_text_characteristics(df, dataset_name):
    """Analisa caracter√≠sticas textuais do dataset"""
    print(f"\n=== AN√ÅLISE TEXTUAL - {dataset_name.upper()} ===")
    
    # Comprimento dos textos
    text_lengths = df['text'].str.len()
    clean_lengths = df['text_clean'].str.len()
    
    print(f"Comprimento m√©dio do texto original: {text_lengths.mean():.1f} caracteres")
    print(f"Comprimento m√©dio do texto limpo: {clean_lengths.mean():.1f} caracteres")
    print(f"Redu√ß√£o m√©dia: {((text_lengths - clean_lengths) / text_lengths * 100).mean():.1f}%")
    
    # Vocabu√°rio √∫nico
    all_words = ' '.join(df['text_clean'].dropna()).split()
    unique_words = len(set(all_words))
    total_words = len(all_words)
    
    print(f"Total de palavras: {total_words:,}")
    print(f"Palavras √∫nicas: {unique_words:,}")
    print(f"Riqueza vocabular: {unique_words/total_words:.3f}")
    
    return {
        'avg_text_length': text_lengths.mean(),
        'avg_clean_length': clean_lengths.mean(),
        'vocab_richness': unique_words/total_words,
        'total_words': total_words,
        'unique_words': unique_words
    }

def analyze_entity_distribution(df, dataset_name):
    """Analisa distribui√ß√£o por entidades"""
    print(f"\n=== AN√ÅLISE POR ENTIDADE - {dataset_name.upper()} ===")
    
    entity_sentiment = df.groupby(['entity', 'sentiment']).size().unstack(fill_value=0)
    entity_counts = df['entity'].value_counts()
    
    print(f"N√∫mero de entidades √∫nicas: {len(entity_counts)}")
    print(f"Entidade mais comum: {entity_counts.index[0]} ({entity_counts.iloc[0]} tweets)")
    print(f"Entidade menos comum: {entity_counts.index[-1]} ({entity_counts.iloc[-1]} tweets)")
    
    # An√°lise de sentimento por entidade
    entity_sentiment_pct = entity_sentiment.div(entity_sentiment.sum(axis=1), axis=0) * 100
    
    print("\nTop 5 entidades com mais tweets positivos:")
    positive_pct = entity_sentiment_pct['Positive'].sort_values(ascending=False).head()
    for entity, pct in positive_pct.items():
        count = entity_sentiment.loc[entity, 'Positive'] if 'Positive' in entity_sentiment.columns else 0
        print(f"  {entity}: {pct:.1f}% ({count} tweets)")
    
    return entity_sentiment, entity_counts

def compare_model_performance_context(train_df, val_df):
    """Compara contextos que podem explicar varia√ß√µes de performance"""
    print("\n=== COMPARA√á√ÉO DE CONTEXTO PARA PERFORMANCE ===")
    
    # 1. Distribui√ß√£o de classes
    train_dist = train_df['sentiment'].value_counts(normalize=True).sort_index()
    val_dist = val_df['sentiment'].value_counts(normalize=True).sort_index()
    
    print("Distribui√ß√£o de classes (treino vs valida√ß√£o):")
    for sentiment in train_dist.index:
        train_pct = train_dist[sentiment] * 100
        val_pct = val_dist[sentiment] * 100 if sentiment in val_dist.index else 0
        diff = abs(train_pct - val_pct)
        print(f"  {sentiment}: Treino {train_pct:.1f}% vs Val {val_pct:.1f}% (dif: {diff:.1f}%)")
    
    # 2. Sobreposi√ß√£o de entidades
    train_entities = set(train_df['entity'].unique())
    val_entities = set(val_df['entity'].unique())
    common_entities = train_entities.intersection(val_entities)
    
    print(f"\nSobreposi√ß√£o de entidades:")
    print(f"  Entidades no treino: {len(train_entities)}")
    print(f"  Entidades na valida√ß√£o: {len(val_entities)}")
    print(f"  Entidades em comum: {len(common_entities)}")
    print(f"  Sobreposi√ß√£o: {len(common_entities)/len(val_entities)*100:.1f}%")
    
    # 3. An√°lise de entidades exclusivas
    train_only = train_entities - val_entities
    val_only = val_entities - train_entities
    
    print(f"\nEntidades exclusivas do treino (top 10): {list(train_only)[:10]}")
    print(f"Entidades exclusivas da valida√ß√£o: {list(val_only)}")
    
    return {
        'common_entities': len(common_entities),
        'train_only_entities': len(train_only),
        'val_only_entities': len(val_only),
        'class_distribution_diff': abs(train_dist - val_dist).mean() * 100
    }

def analyze_text_complexity(df, dataset_name):
    """Analisa complexidade textual"""
    print(f"\n=== AN√ÅLISE DE COMPLEXIDADE - {dataset_name.upper()} ===")
    
    # N√∫mero de palavras por tweet
    word_counts = df['text_clean'].dropna().str.split().str.len()
    
    print(f"M√©dia de palavras por tweet: {word_counts.mean():.1f}")
    print(f"Mediana de palavras por tweet: {word_counts.median():.1f}")
    print(f"M√°ximo de palavras: {word_counts.max()}")
    print(f"M√≠nimo de palavras: {word_counts.min()}")
    
    # Palavras mais frequentes por sentimento
    for sentiment in ['Positive', 'Negative', 'Neutral', 'Irrelevant']:
        if sentiment in df['sentiment'].values:
            sentiment_texts = df[df['sentiment'] == sentiment]['text_clean'].dropna()
            if len(sentiment_texts) > 0:
                words = ' '.join(sentiment_texts).split()
                common_words = Counter(words).most_common(10)
                print(f"\nTop 10 palavras em tweets {sentiment.lower()}:")
                for word, count in common_words:
                    print(f"  {word}: {count}")
    
    return word_counts.describe()

def main():
    """Fun√ß√£o principal de an√°lise"""
    print("üîç AN√ÅLISE DETALHADA DOS DATASETS")
    print("="*50)
    
    # Carregar datasets
    try:
        train_df = pd.read_csv('processed_train.csv')
        val_df = pd.read_csv('processed_validation.csv')
        
        print(f"‚úÖ Datasets carregados com sucesso!")
        print(f"   Treino: {len(train_df):,} amostras")
        print(f"   Valida√ß√£o: {len(val_df):,} amostras")
        
    except FileNotFoundError as e:
        print(f"‚ùå Erro ao carregar datasets: {e}")
        return
    
    # An√°lises
    print("\n" + "="*50)
    
    # 1. An√°lise textual
    train_text_stats = analyze_text_characteristics(train_df, "treino")
    val_text_stats = analyze_text_characteristics(val_df, "valida√ß√£o")
    
    # 2. An√°lise por entidade
    train_entity_sentiment, train_entity_counts = analyze_entity_distribution(train_df, "treino")
    val_entity_sentiment, val_entity_counts = analyze_entity_distribution(val_df, "valida√ß√£o")
    
    # 3. Compara√ß√£o de contexto
    context_comparison = compare_model_performance_context(train_df, val_df)
    
    # 4. An√°lise de complexidade
    train_word_stats = analyze_text_complexity(train_df, "treino")
    val_word_stats = analyze_text_complexity(val_df, "valida√ß√£o")
    
    # Resumo final
    print("\n" + "="*60)
    print("üìä RESUMO DAS DIFEREN√áAS CHAVE")
    print("="*60)
    
    print(f"\nüî¢ Tamanho dos datasets:")
    print(f"   Treino: {len(train_df):,} amostras")
    print(f"   Valida√ß√£o: {len(val_df):,} amostras")
    print(f"   Raz√£o: 1:{len(train_df)/len(val_df):.0f}")
    
    print(f"\nüìà Caracter√≠sticas textuais:")
    print(f"   Comprimento m√©dio treino: {train_text_stats['avg_clean_length']:.1f} chars")
    print(f"   Comprimento m√©dio valida√ß√£o: {val_text_stats['avg_clean_length']:.1f} chars")
    print(f"   Riqueza vocabular treino: {train_text_stats['vocab_richness']:.3f}")
    print(f"   Riqueza vocabular valida√ß√£o: {val_text_stats['vocab_richness']:.3f}")
    
    print(f"\nüéØ Distribui√ß√£o de classes:")
    print(f"   Diferen√ßa m√©dia: {context_comparison['class_distribution_diff']:.1f}%")
    
    print(f"\nüè¢ Sobreposi√ß√£o de entidades:")
    print(f"   Entidades em comum: {context_comparison['common_entities']}")
    print(f"   Entidades exclusivas treino: {context_comparison['train_only_entities']}")
    print(f"   Entidades exclusivas valida√ß√£o: {context_comparison['val_only_entities']}")
    
    print(f"\nüí° POSS√çVEIS EXPLICA√á√ïES PARA VARIA√á√ïES DE PERFORMANCE:")
    print(f"   1. Diferen√ßas no vocabul√°rio e comprimento dos textos")
    print(f"   2. Distribui√ß√£o desigual de classes entre datasets")
    print(f"   3. Entidades diferentes (dom√≠nios distintos)")
    print(f"   4. Tamanho significativamente diferente dos datasets")
    print(f"   5. Complexidade textual vari√°vel")
    
    print(f"\n‚úÖ CONCLUS√ÉO:")
    print(f"   Os modelos provavelmente foram treinados em contextos diferentes,")
    print(f"   com datasets que t√™m caracter√≠sticas distintas. Isso explica as")
    print(f"   varia√ß√µes de performance (KNN 0.94+ vs LinearSVC 0.93+).")

if __name__ == "__main__":
    main()